# K3S - V3

# D√©ployer un cluster K3s sur des VPS Debian 13 chez OVH

> **Objectif :** Monter un cluster K3s haute disponibilit√© sur 3 n≈ìuds master avec etcd distribu√©, sur des VPS OVH. Ce cluster contiendra les briques n√©cessaires pour d√©ployer des applications web (stateless et bases de donn√©es). Le trafic inter-n≈ìuds passe par un r√©seau priv√© Tailscale, et non par les IP publiques.
> 

---

```mermaid
%%{init: {'theme': 'base', 'themeVariables': { 'primaryColor': '#e1f5fe', 'primaryTextColor': '#01579b', 'primaryBorderColor': '#0288d1', 'lineColor': '#546e7a', 'secondaryColor': '#f3e5f5', 'tertiaryColor': '#fff8e1'}}}%%

flowchart TB
    subgraph INTERNET["üåê INTERNET"]
        USER["üë§ Utilisateurs"]
        ADMIN["üîß Administrateur"]
        GIT["üìÅ Repository Git<br/><i>Source de v√©rit√©</i>"]

    end

    subgraph ENTREE["üì• POINTS D'ENTR√âE"]
        direction LR
        HTTPS["üîí HTTPS (443)"]
        TAILSCALE["üîê Tailscale VPN"]
    end

    subgraph CLUSTER["‚ò∏Ô∏è CLUSTER K3S (3 N≈ìuds Masters)"]
        
        subgraph RESEAU["üö¶ ROUTAGE & S√âCURIT√â"]
            TRAEFIK["üöÄ Traefik<br/><i>Ingress Controller</i>"]
            CERTMGR["üìú cert-manager<br/><i>Certificats TLS</i>"]
            KYVERNO["üõ°Ô∏è Kyverno<br/><i>Policies de s√©curit√©</i>"]
            NETPOL["üîí NetworkPolicies<br/><i>Isolation r√©seau</i>"]
        end

        subgraph GITOPS["üîÑ D√âPLOIEMENT GITOPS"]
            ARGOCD["üêô ArgoCD<br/><i>GitOps Controller</i>"]
        end

        subgraph STOCKAGE["üíæ STOCKAGE"]
            LONGHORN["üóÑÔ∏è Longhorn<br/><i>Stockage distribu√©</i>"]
            ETCD["üóÉÔ∏è etcd<br/><i>Base de donn√©es cluster</i>"]
        end

        subgraph MONITORING["üìä OBSERVABILIT√â"]
            PROMETHEUS["üìà Prometheus<br/><i>M√©triques</i>"]
            GRAFANA["üìâ Grafana<br/><i>Dashboards</i>"]
            LOKI["üìù Loki<br/><i>Logs</i>"]
            PROMTAIL["üìã Promtail<br/><i>Agent logs</i>"]
        end

        subgraph APPS["üöÄ APPLICATIONS"]
            APP1["üì¶ App Web 1"]
            APP2["üì¶ App Web 2"]
            DB["üóÑÔ∏è Base de donn√©es"]
        end

    end

    %% Flux clients principaux
    USER ==>|"Requ√™tes web"| HTTPS
    ADMIN ==>|"Administration"| TAILSCALE
    
    GIT ==> ARGOCD
    HTTPS ==> TRAEFIK
    TAILSCALE ==> ARGOCD
    TAILSCALE ==> GRAFANA
    
    TRAEFIK ==> APP1
    TRAEFIK ==> APP2

    %% Styles
    classDef internet fill:#ffebee,stroke:#c62828,stroke-width:2px
    classDef entree fill:#e3f2fd,stroke:#1565c0,stroke-width:2px
    classDef reseau fill:#fff3e0,stroke:#ef6c00,stroke-width:2px
    classDef gitops fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px
    classDef stockage fill:#e8f5e9,stroke:#2e7d32,stroke-width:2px
    classDef monitoring fill:#fff8e1,stroke:#f9a825,stroke-width:2px
    classDef apps fill:#e1f5fe,stroke:#0277bd,stroke-width:2px

    class USER,ADMIN internet
    class HTTPS,TAILSCALE entree
    class TRAEFIK,CERTMGR,KYVERNO,NETPOL reseau
    class ARGOCD,GIT gitops
    class LONGHORN,ETCD stockage
    class PROMETHEUS,GRAFANA,LOKI,PROMTAIL monitoring
    class APP1,APP2,DB apps
```

## Avertissements

**Architecture :** Ce setup utilise 3 masters sans workers d√©di√©s. Les workloads applicatifs tournent sur les masters, ce qui est acceptable pour des charges l√©g√®res mais pas pour de la production intensive. Avec 3 n≈ìuds etcd, le cluster tol√®re la perte d‚Äôun seul n≈ìud. La perte de 2 n≈ìuds entra√Æne la perte du quorum et rend le cluster inop√©rant. Les VPS OVH peuvent avoir des disques limit√©s en IOPS ‚Äî Longhorn peut √™tre impact√© sur des charges I/O intensives.

**S√©curit√© :** Les IP publiques servent uniquement √† l‚Äôexposition HTTP/HTTPS via Traefik. L‚Äôacc√®s SSH est restreint aux IP Tailscale apr√®s la Phase 1. Tout le trafic interne passe par Tailscale. Des r√®gles firewall sont configur√©es √† chaque √©tape. Les secrets (tokens, credentials) ne doivent jamais √™tre commit√©s en clair dans Git.

**Pr√©requis :** 3 VPS Debian 13 chez OVH avec IP publique, un acc√®s root SSH aux 3 machines, un compte GitHub, un compte Tailscale (gratuit pour usage personnel), un nom de domaine avec acc√®s √† la gestion DNS (n√©cessaire √† partir de la Phase 5), et un poste distant sous WSL2 ou Linux pour l‚Äôadministration.

---

## Composants d√©ploy√©s

| Composant | R√¥le | Phase |
| --- | --- | --- |
| **Tailscale** | R√©seau priv√© chiffr√© inter-n≈ìuds | Phase 1 |
| **K3s + etcd** | Cluster Kubernetes haute disponibilit√© | Phase 2 |
| **ArgoCD** | D√©ploiement GitOps | Phase 3 |
| **Longhorn** | StorageClass distribu√©e avec r√©plication | Phase 4 |
| **Traefik** | Ingress Controller | Phase 5 |
| **cert-manager** | Certificats TLS automatis√©s (Let‚Äôs Encrypt) | Phase 6 |
| **Kyverno** | Contr√¥leur d‚Äôadmission (policies de s√©curit√©) | Phase 7 |
| **NetworkPolicies** | Isolation r√©seau inter-namespaces | Phase 8 |
| **Prometheus + Grafana** | Monitoring infrastructure et dashboards | Phase 9 |
| **Loki + Promtail** | Agr√©gation et consultation des logs | Phase 10 |

---

## Phase 1 : Pr√©paration de l‚Äôinfrastructure

### √âtape 1 ‚Äî Pr√©paration et s√©curisation des n≈ìuds

**√Ä r√©aliser sur les 3 n≈ìuds.**

### Mise √† jour et d√©pendances

```bash
sudo apt update && sudo apt upgrade -y

# D√©pendances de base
sudo apt install -y curl wget gnupg2 software-properties-common apt-transport-https ca-certificates

# D√©pendances Longhorn (install√©es maintenant pour √©viter les probl√®mes plus tard)
sudo apt install -y open-iscsi nfs-common util-linux

# Activer iSCSI
sudo systemctl enable --now iscsid

# Charger le module iscsi_tcp et le rendre persistant
sudo modprobe iscsi_tcp
echo "iscsi_tcp" | sudo tee -a /etc/modules
```

V√©rifications :

```bash
sudo systemctl is-active iscsid
# R√©sultat attendu : active

lsmod | grep iscsi_tcp
# R√©sultat attendu : iscsi_tcp suivi de chiffres
```

### D√©sactivation du swap

```bash
sudo swapoff -a
sudo sed -i '/swap/d' /etc/fstab

# V√©rification : la ligne Swap doit afficher 0
free -h | grep Swap
```

### Configuration du firewall UFW

> Adapter le port SSH √† votre configuration si n√©cessaire.
> 

```bash
sudo apt install -y ufw

sudo ufw default deny incoming
sudo ufw default allow outgoing

# Autoriser SSH AVANT d'activer le firewall
# Phase initiale : SSH depuis partout (sera restreint √† l'√©tape 3)
sudo ufw allow 22/tcp comment 'SSH temporaire'

# Autoriser tout le trafic sur l'interface Tailscale (configur√©e √† l'√©tape 2)
sudo ufw allow in on tailscale0 comment 'Tailscale VPN'

sudo ufw enable
```

V√©rification :

```bash
sudo ufw status verbose
# R√©sultat attendu : Status: active, avec les r√®gles SSH et tailscale0
```

### Optimisations syst√®me pour Kubernetes

Par d√©faut, Linux n‚Äôest pas configur√© pour faire tourner un orchestrateur de conteneurs. Ces param√®tres corrigent √ßa :

- **`br_netfilter`** ‚Äî Permet √† Linux d‚Äôappliquer les r√®gles iptables au trafic transitant par les bridges r√©seau virtuels cr√©√©s par Kubernetes. Indispensable pour le fonctionnement des NetworkPolicies et du routage des Services.
- **`net.ipv4.ip_forward`** ‚Äî Autorise le transit de trafic entre interfaces r√©seau, comme un routeur. Sans √ßa, un pod sur le n≈ìud 1 ne peut pas communiquer avec un pod sur le n≈ìud 2.
- **`net.bridge.bridge-nf-call-iptables`** ‚Äî Compl√©ment de `br_netfilter`. Soumet le trafic des bridges aux r√®gles iptables pour que Flannel (r√©seau overlay de K3s) route correctement les paquets entre pods.
- **`fs.inotify.max_user_instances`** et **`max_user_watches`** ‚Äî Kubernetes surveille en permanence des milliers de fichiers (configs, secrets, logs). Les valeurs par d√©faut sont trop basses et provoquent des erreurs ‚Äútoo many open files‚Äù quand le nombre de pods augmente.

```bash
sudo modprobe br_netfilter
echo "br_netfilter" | sudo tee -a /etc/modules

cat <<EOF | sudo tee /etc/sysctl.d/99-kubernetes.conf
fs.inotify.max_user_instances=8192
fs.inotify.max_user_watches=524288
net.ipv4.ip_forward=1
net.bridge.bridge-nf-call-iptables=1
EOF

sudo sysctl --system
```

V√©rification :

```bash
sudo sysctl net.ipv4.ip_forward net.bridge.bridge-nf-call-iptables
# R√©sultat attendu : les deux valeurs √† 1
```

> Les param√®tres sont plac√©s dans `/etc/sysctl.d/99-kubernetes.conf` plut√¥t que dans `/etc/sysctl.conf` pour suivre la convention Debian de configuration modulaire.
> 

---

# √âtape 2 ‚Äî Configuration de Tailscale

Tailscale cr√©e un r√©seau priv√© chiffr√© (WireGuard) entre les n≈ìuds sans exposer les ports sensibles sur Internet. Il simplifie la communication inter-n≈ìuds et offre une authentification forte.

S'authentifier ou cr√©er un compte sur [Tailscale](https://tailscale.com/) au pr√©alable.

---

## Installation sur WSL

```bash
curl -fsSL https://tailscale.com/install.sh | sh
sudo tailscale up --advertise-tags=tag:ssh
# Une URL d'authentification sera affich√©e.
```

---

## Configuration des tags et ACL sur le dashboard Tailscale

Dans l'onglet **Access Controls**, remplacer le contenu par la politique suivante :

```json
{
  "tagOwners": {
    "tag:node": ["autogroup:admin"],
    "tag:ssh":  ["autogroup:admin"]
  },

  "acls": [
    // Trafic inter-noeuds (K3s, etcd, Flannel, Longhorn, Prometheus)
    {
      "action": "accept",
      "src":    ["tag:node"],
      "dst":    ["tag:node:6443", "tag:node:10250", "tag:node:2379-2380",
                 "tag:node:51820", "tag:node:8472",  "tag:node:9500",
                 "tag:node:9501-9502", "tag:node:8300", "tag:node:10255",
                 "tag:node:9090"]
    },

    // SSH et kubectl depuis WSL
    {
      "action": "accept",
      "src":    ["tag:ssh"],
      "dst":    ["tag:node:22", "tag:node:6443"]
    }
  ],

  "ssh": [
    {
      "action": "accept",
      "src":    ["tag:ssh"],
      "dst":    ["tag:node"],
      "users":  ["debian"]
    }
  ]
}
```

## Installation sur les 3 n≈ìuds

**√Ä r√©aliser sur les 3 n≈ìuds.**

```bash
curl -fsSL https://tailscale.com/install.sh | sh
sudo tailscale up --advertise-tags=tag:node --ssh
# Une URL d'authentification sera affich√©e.
# Ouvrir cette URL dans un navigateur pour lier le n≈ìud √† votre compte Tailscale.
```

---

---

## Configuration sur le dashboard Tailscale

Sur [le dashboard Tailscale](https://login.tailscale.com/admin/machines) :

1. Renommer les machines : `k3s-master-1`, `k3s-master-2`, `k3s-master-3`
2. D√©sactiver l'expiration des cl√©s pour chaque machine (Machine settings ‚Üí Disable key expiry)
3. V√©rifier que chaque n≈ìud porte bien le tag `tag:node` et que WSL porte `tag:ssh`

---

## V√©rification de la connectivit√©

```bash
tailscale status

# R√©cup√©rer et noter votre IP Tailscale
tailscale ip -4

# Tester la connectivit√© vers les autres n≈ìuds
ping -c 3 <IP_TAILSCALE_AUTRE_NOEUD>
# R√©sultat attendu : 0% packet loss
```

> Les IP Tailscale sont dans la plage `100.64.0.0/10`. Elles sont stables tant que la machine reste dans votre r√©seau Tailscale. Notez les 3 IP pour les √©tapes suivantes.
> 

---

## Test du firewall

V√©rifier que les ports K3s ne sont **pas** accessibles depuis Internet :

```bash
nc -zv <IP_PUBLIQUE_NOEUD1> 6443
# R√©sultat attendu : Connection refused ou timeout
```

---

### √âtape 3 ‚Äî Durcissement SSH

Une fois Tailscale fonctionnel sur les 3 n≈ìuds **et sur votre poste de travail**, l‚Äôacc√®s SSH est restreint au r√©seau Tailscale. Cela supprime le vecteur d‚Äôattaque principal (bruteforce SSH sur IP publique).

**√Ä r√©aliser depuis WSL, vers les 3 n≈ìuds.**

> **Pr√©requis critique :** Avant de modifier les r√®gles SSH, v√©rifier que vous pouvez vous connecter aux n≈ìuds via leur IP Tailscale depuis votre poste.
> 

```bash
# Depuis votre poste ‚Äî tester la connexion SSH via Tailscale
ssh debian@<NOM_DE_LA_MACHINE_TAILSCALE>
# R√©sultat attendu : SSH via Tailscale OK
```

### Configuration sshd

> Sur les 3 noeuds:
> 

```bash
# Sauvegarder la configuration actuelle
sudo cp /etc/ssh/sshd_config /etc/ssh/sshd_config.bak

cat <<EOF | sudo tee /etc/ssh/sshd_config.d/99-hardening.conf
# --- Restrictions r√©seau ---
# √âcouter uniquement sur l'IP Tailscale du n≈ìud
ListenAddress $(tailscale ip -4)

# --- Authentification ---
PermitRootLogin prohibit-password
PubkeyAuthentication yes
PasswordAuthentication no
PermitEmptyPasswords no
AuthenticationMethods publickey
MaxAuthTries 3
MaxSessions 3

# --- D√©sactiver les m√©thodes inutiles ---
KbdInteractiveAuthentication no
ChallengeResponseAuthentication no
UsePAM yes

# --- Durcissement protocole ---
X11Forwarding no
AllowTcpForwarding no
AllowAgentForwarding no
PermitTunnel no
GatewayPorts no
PrintMotd no

# --- Timeout et keepalive ---
ClientAliveInterval 300
ClientAliveCountMax 2
LoginGraceTime 30
EOF
```

> **`ListenAddress` sur l‚ÄôIP Tailscale** fait que sshd n‚Äô√©coute plus du tout sur l‚ÄôIP publique. C‚Äôest le durcissement le plus efficace : m√™me si le port 22 est ouvert dans UFW, aucune connexion SSH ne peut arriver par l‚ÄôIP publique.
> 

### V√©rifier et appliquer

```bash
# Valider la syntaxe avant de red√©marrer
sudo sshd -t
# R√©sultat attendu : aucune sortie (= pas d'erreur)

# Red√©marrer sshd
sudo systemctl restart sshd
```

### Mettre √† jour les r√®gles firewall

```bash
# Supprimer l'ancienne r√®gle SSH ouverte √† tous
sudo ufw status numbered
```

```bash
# Identifier les r√®gles "SSH temporaire" (il y en a 2 : IPv4 et IPv6)
sudo ufw delete <num√©ro_ipv6>
```

```bash
sudo ufw delete <num√©ro_ipv4>
```

```bash
# Restreindre SSH au r√©seau Tailscale uniquement
sudo ufw allow from 100.64.0.0/10 to any port 22 proto tcp comment 'SSH via Tailscale'
sudo ufw reload
```

### V√©rification

```bash
# Depuis votre poste ‚Äî via IP publique (doit √©chouer)
ssh -o ConnectTimeout=5 debian@<IP_PUBLIQUE_NOEUD>
# R√©sultat attendu : Connection timed out ou Connection refused
```

> **Proc√©dure de secours :** Si vous perdez l‚Äôacc√®s SSH via Tailscale, utilisez la console VNC/KVM de l‚Äôinterface OVH pour restaurer `/etc/ssh/sshd_config.bak` et red√©marrer sshd.
> 

### √âtat attendu des r√®gles UFW apr√®s durcissement SSH

```
To                         Action      From
--                         ------      ----
Anywhere on tailscale0     ALLOW IN    Anywhere                   # Tailscale VPN
22/tcp                     ALLOW IN    100.64.0.0/10              # SSH via Tailscale
Anywhere (v6) on tailscale0 ALLOW IN    Anywhere (v6)              # Tailscale VPN
```

---

## Phase 2 : Installation du cluster K3s

### √âtape 4 ‚Äî Installation de K3s sur le premier master

**Sur le n≈ìud 1 uniquement.**

### Pr√©paration des variables

```bash
export IP_TAILSCALE=$(tailscale ip -4)
export IP_PUBLIQUE=$(curl -4 -s ifconfig.me)

echo "IP Tailscale :$IP_TAILSCALE"
echo "IP Publique  :$IP_PUBLIQUE"
```

### Installation

```bash
curl -sfL https://get.k3s.io | sh -s - server \
  --cluster-init \
  --disable=traefik \
  --disable=servicelb \
  --node-ip=$IP_TAILSCALE \
  --advertise-address=$IP_TAILSCALE \
  --tls-san=$IP_TAILSCALE \
  --tls-san=$IP_PUBLIQUE \
  --flannel-iface=tailscale0 \
  --write-kubeconfig-mode=644
  --etcd-expose-metrics
```

**Explication des options :**

| Option | R√¥le |
| --- | --- |
| `--cluster-init` | Active etcd en mode haute disponibilit√© |
| `--disable=traefik` | D√©sactive le Traefik int√©gr√© (d√©ploy√© via ArgoCD en Phase 5) |
| `--disable=servicelb` | D√©sactive Klipper. Traefik utilisera `hostPort` |
| `--node-ip` | IP pour la communication inter-n≈ìuds (Tailscale) |
| `--advertise-address` | IP annonc√©e aux autres n≈ìuds (Tailscale) |
| `--tls-san` | IP autoris√©es dans le certificat TLS de l‚ÄôAPI (Tailscale + publique pour kubectl distant) |
| `--flannel-iface` | Force Flannel √† utiliser l‚Äôinterface Tailscale |
| `--write-kubeconfig-mode` | Permet la lecture du kubeconfig sans sudo. Acceptable sur des VPS mono-utilisateur |
| `--etcd-expose-metrics` | Expose les m√©triques etcd sur `127.0.0.1:2379/metrics` pour Prometheus |

### V√©rification

```bash
sudo systemctl is-active k3s
# R√©sultat attendu : active

# Attendre que le n≈ìud soit Ready (30-60 secondes)
sudo k3s kubectl get node
# R√©sultat attendu : STATUS = Ready

sudo k3s kubectl get pods -A
# Tous les pods doivent √™tre Running ou Completed
```

### R√©cup√©rer le token K3s

```bash
sudo cat /var/lib/rancher/k3s/server/node-token
```

**Sauvegarder ces informations pour les prochaines √©tapes :**

```
IP Tailscale n≈ìud 1 : <noter_ici>
Token K3s           : <noter_ici>
```

### Snapshot etcd initial

```bash
sudo k3s etcd-snapshot save --name initial-setup
sudo k3s etcd-snapshot ls
```

---

### √âtape 5 ‚Äî Installation de K3s sur les masters 2 et 3

**R√©p√©ter cette proc√©dure sur les n≈ìuds 2 et 3.**

### Pr√©paration

```bash
export IP_TAILSCALE=$(tailscale ip -4)
export IP_PUBLIQUE=$(curl -4 -s ifconfig.me)

echo "IP Tailscale :$IP_TAILSCALE"
echo "IP Publique  :$IP_PUBLIQUE"

# Variables du cluster (r√©cup√©r√©es du n≈ìud 1)
export IP_NOEUD1=<IP_TAILSCALE_DU_NOEUD1>
export K3S_TOKEN=<TOKEN_DU_NOEUD1>

# Tester la connectivit√© vers le n≈ìud 1
ping -c 3 $IP_NOEUD1
nc -zv $IP_NOEUD1 6443
# R√©sultat attendu : Connection to ... 6443 port [tcp/*] succeeded!
```

### Installation

```bash
curl -sfL https://get.k3s.io | sh -s - server \
  --server https://$IP_NOEUD1:6443 \
  --token $K3S_TOKEN \
  --disable=traefik \
  --disable=servicelb \
  --node-ip=$IP_TAILSCALE \
  --advertise-address=$IP_TAILSCALE \
  --tls-san=$IP_TAILSCALE \
  --tls-san=$IP_PUBLIQUE \
  --flannel-iface=tailscale0 \
  --write-kubeconfig-mode=644
  --etcd-expose-metrics
```

### V√©rification

```bash
sudo systemctl is-active k3s
# R√©sultat attendu : active

sudo k3s kubectl get nodes
# R√©sultat attendu : le nouveau n≈ìud appara√Æt en Ready (peut prendre 30-60s)
```

---

### √âtape 6 ‚Äî Validation du cluster

**Sur n‚Äôimporte quel n≈ìud.**

### V√©rification des n≈ìuds

```bash
sudo k3s kubectl get nodes -o wide
```

R√©sultat attendu :

```
NAME           STATUS   ROLES                       AGE   VERSION
k3s-master-1   Ready    control-plane,etcd,master   10m   v1.34.x+k3s1
k3s-master-2   Ready    control-plane,etcd,master   5m    v1.34.x+k3s1
k3s-master-3   Ready    control-plane,etcd,master   2m    v1.34.x+k3s1
```

Les 3 n≈ìuds doivent avoir le statut `Ready` et les r√¥les `control-plane,etcd,master`.

### V√©rification des pods syst√®me et DNS

```bash
sudo k3s kubectl get pods -A
# Tous les pods doivent √™tre Running

# CoreDNS
sudo k3s kubectl get pods -n kube-system -l k8s-app=kube-dns

# Test de r√©solution DNS interne
sudo k3s kubectl run dns-test --image=busybox:1.36 --rm -it --restart=Never -- nslookup kubernetes.default.svc.cluster.local
# R√©sultat attendu : adresse IP du service kubernetes (10.43.0.1 par d√©faut)
```

### Snapshot etcd post-validation

```bash
sudo k3s etcd-snapshot save --name cluster-validated
```

### Test de r√©silience (optionnel)

Ce test v√©rifie le comportement du cluster lors de la perte de n≈ìuds.

```bash
# Sur un n≈ìud : arr√™ter K3s
sudo systemctl stop k3s

# Sur un autre n≈ìud : v√©rifier que le cluster fonctionne toujours
sudo k3s kubectl get pods -A
# R√©sultat attendu : le cluster r√©pond normalement

# Sur un second n≈ìud : arr√™ter K3s (2 n≈ìuds sur 3 down)
sudo systemctl stop k3s

# Sur le dernier n≈ìud :
sudo k3s kubectl get pods -A
# R√©sultat attendu : erreur ‚Äî etcd a perdu son quorum, c'est le comportement attendu

# Red√©marrer les n≈ìuds arr√™t√©s
sudo systemctl start k3s
# Attendre 30-60 secondes puis v√©rifier le retour √† la normale
sudo k3s kubectl get nodes
```

---

## Phase 3 : Configuration de l‚Äôacc√®s distant et ArgoCD

### √âtape 7 ‚Äî Configurer kubectl pour un acc√®s distant

L‚Äôobjectif est d‚Äôadministrer le cluster depuis un poste de travail sans passer par SSH.

### R√©cup√©rer le kubeconfig

Sur le n≈ìud 1 :

```bash
sudo cat /etc/rancher/k3s/k3s.yaml
```

Dans le contenu copi√©, remplacer `127.0.0.1` par l‚ÄôIP Tailscale du n≈ìud 1.

### Installer kubectl sur le poste de travail

```bash
curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.31/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.31/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubectl
```

### Configurer le kubeconfig

```bash
mkdir -p ~/.kube
nano ~/.kube/config
# Coller le contenu du kubeconfig modifi√©

chmod 600 ~/.kube/config
```

### V√©rification et renommage du contexte

```bash
kubectl get nodes -o wide
# R√©sultat attendu : les 3 n≈ìuds en Ready

# Renommer le contexte (recommand√©)
kubectl config rename-context default k3s-ovh
```

---

### √âtape 8 ‚Äî D√©ployer ArgoCD

ArgoCD fournit un d√©ploiement GitOps o√π Git est la source de v√©rit√© unique, avec synchronisation automatique des ressources Kubernetes et une interface web de visualisation.

**Depuis votre poste de travail avec kubectl configur√©.**

### Installation

```bash
kubectl create namespace argocd
kubectl apply -n argocd --server-side --force-conflicts -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml

# Attendre que tous les pods soient Running (2-3 minutes)
kubectl get pods -n argocd -w
# Ctrl+C une fois tous les pods Running
```

### Exposition temporaire via NodePort

> **Temporaire.** Cette exposition sera remplac√©e par un Ingress Traefik avec HTTPS en Phase 6 (√©tape 18).
> 

```bash
kubectl patch svc argocd-server -n argocd -p '{
  "spec": {
    "type": "NodePort",
    "ports": [
      {
        "name": "https",
        "port": 443,
        "targetPort": 8080,
        "nodePort": 30443,
        "protocol": "TCP"
      }
    ]
  }
}'
```

Ouvrir le port sur le firewall du n≈ìud 1 :

```bash
MY_PUBLIC_IP="<votre_IP_publique>"
sudo ufw allow from $MY_PUBLIC_IP to any port 30443 proto tcp comment 'ArgoCD NodePort temporaire'
```

### Connexion

```bash
# R√©cup√©rer le mot de passe admin
kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath="{.data.password}" | base64 -d && echo
```

- URL : `https://<IP_PUBLIQUE_NOEUD1>:30443`
- Username : `admin`
- Password : le mot de passe r√©cup√©r√© ci-dessus

Le navigateur affichera un avertissement de certificat auto-sign√© ‚Äî c‚Äôest normal, accepter et continuer. **Changer le mot de passe imm√©diatement** via User Info ‚Üí Update Password.

---

### √âtape 9 ‚Äî Configurer le repository Git

### Cr√©er le repository

1. [Cr√©er un nouveau repository priv√©](https://github.com/new) sur GitHub (ex : `k3s-gitops`)
2. L‚Äôinitialiser avec un README

### Cr√©er un Personal Access Token (PAT)

1. GitHub ‚Üí Settings ‚Üí Developer Settings ‚Üí Personal Access Tokens ‚Üí Tokens (classic)
2. Generate new token (classic)
3. Note : `ArgoCD k3s cluster`
4. Expiration : 90 days (ou selon votre politique)
5. Scope : `repo` (full control of private repositories)
6. **Copier et sauvegarder le token imm√©diatement (affich√© une seule fois)**

### Connecter le repository dans ArgoCD

Sur l‚Äôinterface web ArgoCD : Settings ‚Üí Repositories ‚Üí Connect Repo

| Champ | Valeur |
| --- | --- |
| Connection method | VIA HTTPS |
| Type | git |
| Name | github-k3s-gitops |
| Project | default |
| Repository URL | `https://github.com/<username>/k3s-gitops.git` |
| Username | votre username GitHub |
| Password | votre token PAT |

V√©rifier que le statut est **Successful** (ic√¥ne verte).

---

## Phase 4 : D√©ploiement de Longhorn via GitOps

### √âtape 10 ‚Äî V√©rifier les pr√©requis Longhorn sur les n≈ìuds

> Les d√©pendances ont √©t√© install√©es √† l‚Äô√©tape 1. Cette √©tape valide leur bon fonctionnement.
> 

**Sur les 3 n≈ìuds :**

```bash
sudo systemctl is-active iscsid
# R√©sultat attendu : active

lsmod | grep iscsi_tcp
# R√©sultat attendu : iscsi_tcp suivi de chiffres

dpkg -l | grep -E 'open-iscsi|nfs-common'
# R√©sultat attendu : les deux paquets list√©s avec statut "ii"
```

**Validation automatique via les DaemonSets Longhorn (optionnel mais recommand√©) :**

```bash
# V√©rification iSCSI
kubectl apply -f https://raw.githubusercontent.com/longhorn/longhorn/v1.7.2/deploy/prerequisite/longhorn-iscsi-installation.yaml
kubectl get pods -l app=longhorn-iscsi-installation -o wide --watch
kubectl logs -l app=longhorn-iscsi-installation -c iscsi-installation
# R√©sultat attendu : "iscsi install successfully" sur chaque n≈ìud
```

```bash
kubectl delete -f https://raw.githubusercontent.com/longhorn/longhorn/v1.7.2/deploy/prerequisite/longhorn-iscsi-installation.yaml
```

```bash
# V√©rification NFS
kubectl apply -f https://raw.githubusercontent.com/longhorn/longhorn/v1.7.2/deploy/prerequisite/longhorn-nfs-installation.yaml
kubectl get pods -l app=longhorn-nfs-installation -o wide --watch
kubectl logs -l app=longhorn-nfs-installation -c nfs-installation
# R√©sultat attendu : "nfs install successfully" sur chaque n≈ìud
```

```bash
kubectl delete -f https://raw.githubusercontent.com/longhorn/longhorn/v1.7.2/deploy/prerequisite/longhorn-nfs-installation.yaml
```

---

### √âtape 11 ‚Äî Pr√©parer le repository Git pour Longhorn

**Depuis votre poste de travail.**

### Cloner et structurer le repository

```bash
cd ~
git clone https://github.com/<username>/k3s-gitops.git
cd k3s-gitops

mkdir -p infrastructure/longhorn
mkdir -p infrastructure/traefik/base
mkdir -p infrastructure/traefik/overlays/production
mkdir -p infrastructure/network-policies
mkdir -p applications
mkdir -p argocd-apps
```

### Installer les outils sur votre poste

```bash
# Helm
curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash

# Kustomize
curl -s "https://raw.githubusercontent.com/kubernetes-sigs/kustomize/master/hack/install_kustomize.sh" | bash
sudo mv kustomize /usr/local/bin/
```

### Faciliter les configurations

Pour permettre de suivre correctement les documentations ci-dessous, d√©finir la variable d‚Äôenvironnement suivante sur votre poste WSL / Linux :

```bash
GIT_URL="URL_DE_VOTRE_REPO_GIT"
```

### Cr√©er le values.yaml Longhorn

```bash
cat > infrastructure/longhorn/values.yaml <<'EOF'
# Configuration Longhorn pour K3s 3 masters
csi:
  kubeletRootDir: "/var/lib/kubelet"

persistence:
  defaultClass: true
  defaultClassReplicaCount: 2  # 2 r√©plicas sur 3 n≈ìuds (bon compromis espace/redondance)
  reclaimPolicy: Retain

defaultSettings:
  backupTarget: ""  # √Ä configurer plus tard pour les backups S3/NFS
  defaultReplicaCount: 2
  guaranteedEngineManagerCPU: 5
  guaranteedReplicaManagerCPU: 5

service:
  ui:
    type: ClusterIP  # Accessible uniquement via Ingress (sera configur√© plus tard)

longhornManager:
  resources:
    requests:
      cpu: 100m
      memory: 128Mi
    limits:
      cpu: 500m
      memory: 512Mi

longhornDriver:
  resources:
    requests:
      cpu: 100m
      memory: 128Mi
    limits:
      cpu: 500m
      memory: 256Mi

metrics:
  serviceMonitor:
    enabled: false
EOF
```

### Cr√©er l‚ÄôApplication ArgoCD pour Longhorn

```bash
cat > argocd-apps/longhorn.yaml <<EOF
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: longhorn
  namespace: argocd
  finalizers:
    - resources-finalizer.argocd.argoproj.io
spec:
  project: default

  sources:
    - repoURL: 'https://charts.longhorn.io'
      targetRevision: 1.7.2
      chart: longhorn
      helm:
        releaseName: longhorn
        valueFiles:
          - '\$values/infrastructure/longhorn/values.yaml'

    - repoURL: '$GIT_URL'
      targetRevision: main
      ref: values

  destination:
    server: 'https://kubernetes.default.svc'
    namespace: longhorn-system

  syncPolicy:
    automated:
      prune: true
      selfHeal: true
      allowEmpty: false
    syncOptions:
      - CreateNamespace=true
      - ServerSideApply=true
    retry:
      limit: 5
      backoff:
        duration: 5s
        factor: 2
        maxDuration: 3m
EOF

```

### Pousser sur Git

```bash
git add .
git commit -m "Add Longhorn configuration"
git push origin main
```

---

### √âtape 12 ‚Äî D√©ployer Longhorn via ArgoCD

**Depuis votre poste de travail.**

```bash
kubectl apply -f argocd-apps/longhorn.yaml

# Suivre le d√©ploiement (3-5 minutes)
kubectl get pods -n longhorn-system -w
```

<aside>
‚õî

Si le job pre-upgrade rend des events : error looking up service account

- Terminer la synchronisation en cours
- Synchroniser seulement les 3 Service Account
- Relancer un Synchronisation compl√®te
</aside>

### V√©rification

```bash
kubectl get pods -n longhorn-system
kubectl get daemonsets -n longhorn-system
kubectl get deployments -n longhorn-system
kubectl get storageclass
# R√©sultat attendu : longhorn (default) et local-path (default)
```

### Supprimer la StorageClass local-path

Une fois Longhorn op√©rationnel :

```bash
# V√©rifier qu'aucun PVC n'est pr√©sent et n'utilise local-path
kubectl get pvc -A

# Retirer le caract√®re default
kubectl patch storageclass local-path \
  -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"false"}}}'

kubectl delete storageclass local-path

# V√©rification : seule longhorn doit rester comme default
kubectl get storageclass
```

### Test de la StorageClass Longhorn

```bash
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: test-longhorn-pvc
  namespace: default
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: longhorn
  resources:
    requests:
      storage: 1Gi
EOF

# V√©rifier que le PVC est Bound
kubectl get pvc test-longhorn-pvc

cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: test-longhorn-pod
  namespace: default
spec:
  containers:
  - name: test
    image: nginx:alpine
    volumeMounts:
    - name: test-volume
      mountPath: /data
  volumes:
  - name: test-volume
    persistentVolumeClaim:
      claimName: test-longhorn-pvc
EOF

kubectl get pod test-longhorn-pod -w
# Attendre Running

kubectl exec test-longhorn-pod -it -- sh -c 'echo "Longhorn works!" > /data/test.txt && cat /data/test.txt'
# R√©sultat attendu : Longhorn works!

# Nettoyer
kubectl delete pod test-longhorn-pod
kubectl delete pvc test-longhorn-pvc
```

---

## Phase 5 : D√©ploiement de Traefik Ingress Controller

### √âtape 13 ‚Äî Pr√©parer le d√©ploiement de Traefik

Traefik est d√©ploy√© en DaemonSet sur tous les n≈ìuds. Les ports 80 et 443 sont expos√©s via `hostPort` pour recevoir le trafic HTTP/HTTPS directement (le ServiceLB de K3s est d√©sactiv√© depuis l‚Äô√©tape 4).

**Depuis votre poste de travail.**

### Cr√©er le values.yaml Traefik

```bash
cat > infrastructure/traefik/base/values.yaml <<'EOF'
# Configuration Traefik pour K3s avec 3 masters

deployment:
  kind: DaemonSet

ports:
  web:
    port: 8000
    exposedPort: 80
    expose:
      default: true
    protocol: TCP
    hostPort: 80
    redirectTo:
      port: websecure
  websecure:
    port: 8443
    exposedPort: 443
    expose:
      default: true
    protocol: TCP
    hostPort: 443
    tls:
      enabled: true
  metrics:
    port: 9100
    expose:
      default: false
    protocol: TCP

service:
  enabled: true
  type: ClusterIP

providers:
  kubernetesCRD:
    enabled: true
    allowCrossNamespace: true
  kubernetesIngress:
    enabled: true
    publishedService:
      enabled: true

logs:
  general:
    level: INFO
  access:
    enabled: true
    filters:
      statusCodes: "400-499,500-599"

metrics:
  prometheus:
    enabled: false
    addEntryPointsLabels: true
    addRoutersLabels: true
    addServicesLabels: true
    service:
      enabled: false
    serviceMonitor:
      enabled: false
      jobLabel: traefik
      namespaceSelector: {}
      metricRelabelings: []
      relabelings: []

resources:
  requests:
    cpu: 100m
    memory: 128Mi
  limits:
    cpu: 500m
    memory: 512Mi

persistence:
  enabled: false

ingressRoute:
  dashboard:
    enabled: false

securityContext:
  capabilities:
    drop: [ALL]
    add: [NET_BIND_SERVICE]
  readOnlyRootFilesystem: true
  runAsGroup: 0
  runAsNonRoot: false
  runAsUser: 0

podSecurityContext:
  fsGroup: 65532

globalArguments:
  - "--global.checknewversion=false"
  - "--global.sendanonymoususage=false"

additionalArguments:
  - "--api.dashboard=true"

env:
  - name: TZ
    value: Europe/Paris

tolerations:
  - key: node-role.kubernetes.io/control-plane
    operator: Exists
    effect: NoSchedule
  - key: node-role.kubernetes.io/master
    operator: Exists
    effect: NoSchedule

affinity:
  podAntiAffinity:
    preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm:
          labelSelector:
            matchExpressions:
              - key: app.kubernetes.io/name
                operator: In
                values:
                  - traefik
          topologyKey: kubernetes.io/hostname
EOF
```

> **Note sur le securityContext :** `hostPort` sur les ports < 1024 n√©cessite les capabilities `NET_BIND_SERVICE` et un utilisateur root. Les valeurs `runAsUser: 0` et `runAsNonRoot: false` sont requises pour que Traefik puisse se binder aux ports 80 et 443 directement sur l‚Äôh√¥te.
> 

### Cr√©er les middlewares

```bash
cat > infrastructure/traefik/base/middlewares.yaml <<'EOF'
---
apiVersion: traefik.io/v1alpha1
kind: Middleware
metadata:
  name: security-headers
spec:
  headers:
    frameDeny: true
    sslRedirect: true
    browserXssFilter: true
    contentTypeNosniff: true
    forceSTSHeader: true
    stsIncludeSubdomains: true
    stsPreload: true
    stsSeconds: 31536000
    customFrameOptionsValue: "SAMEORIGIN"
    customResponseHeaders:
      X-Robots-Tag: "none,noarchive,nosnippet,notranslate,noimageindex"
      server: ""
---
apiVersion: traefik.io/v1alpha1
kind: Middleware
metadata:
  name: compression
spec:
  compress:
    excludedContentTypes:
      - text/event-stream
---
apiVersion: traefik.io/v1alpha1
kind: Middleware
metadata:
  name: rate-limit
spec:
  rateLimit:
    average: 100
    burst: 200
---
apiVersion: traefik.io/v1alpha1
kind: Middleware
metadata:
  name: rate-limit-api
spec:
  rateLimit:
    average: 20
    burst: 50
---
apiVersion: traefik.io/v1alpha1
kind: Middleware
metadata:
  name: retry
spec:
  retry:
    attempts: 3
    initialInterval: 100ms
---
apiVersion: traefik.io/v1alpha1
kind: Middleware
metadata:
  name: default-security-chain
spec:
  chain:
    middlewares:
      - name: security-headers
      - name: compression
      - name: rate-limit
EOF
```

### Cr√©er les kustomization.yaml

```bash
cat > infrastructure/traefik/base/kustomization.yaml <<'EOF'
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
namespace: traefik
resources:
  - middlewares.yaml
EOF

cat > infrastructure/traefik/overlays/production/kustomization.yaml <<'EOF'
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
namespace: traefik
resources:
  - ../../base
EOF
```

### Valider le build Kustomize

```bash
kustomize build infrastructure/traefik/overlays/production
# R√©sultat attendu : les manifestes YAML des middlewares, sans erreur
```

### Cr√©er l‚ÄôApplication ArgoCD pour Traefik

```bash
cat > argocd-apps/traefik.yaml <<EOF
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: traefik
  namespace: argocd
  finalizers:
    - resources-finalizer.argocd.argoproj.io
spec:
  project: default

  sources:
    - repoURL: 'https://traefik.github.io/charts'
      targetRevision: 31.1.1
      chart: traefik
      helm:
        releaseName: traefik
        valueFiles:
          - '\$values/infrastructure/traefik/base/values.yaml'

    - repoURL: '$GIT_URL'
      targetRevision: main
      ref: values

    - repoURL: '$GIT_URL'
      targetRevision: main
      path: infrastructure/traefik/overlays/production

  destination:
    server: 'https://kubernetes.default.svc'
    namespace: traefik

  syncPolicy:
    automated:
      prune: true
      selfHeal: true
      allowEmpty: false
    syncOptions:
      - CreateNamespace=true
      - ServerSideApply=true
    retry:
      limit: 5
      backoff:
        duration: 5s
        factor: 2
        maxDuration: 3m
EOF
```

---

### √âtape 14 ‚Äî Configurer le firewall pour Traefik

**Sur les 3 n≈ìuds :**

```bash
sudo ufw allow 80/tcp comment 'Traefik HTTP'
sudo ufw allow 443/tcp comment 'Traefik HTTPS'
sudo ufw reload
```

**√âtat attendu des r√®gles UFW √† ce stade :**

```
To                         Action      From
--                         ------      ----
Anywhere on tailscale0     ALLOW       Anywhere                   # Tailscale VPN
22/tcp                     ALLOW       100.64.0.0/10              # SSH via Tailscale
30443/tcp                  ALLOW       152.228.129.67             # ArgoCD NodePort tem
oraire
80/tcp                     ALLOW       Anywhere                   # Traefik HTTP
443/tcp                    ALLOW       Anywhere                   # Traefik HTTPS
Anywhere (v6) on tailscale0 ALLOW       Anywhere (v6)             # Tailscale VPN
80/tcp (v6)                ALLOW       Anywhere (v6)              # Traefik HTTP
443/tcp (v6)               ALLOW       Anywhere (v6)              # Traefik HTTPS
```

---

### √âtape 15 ‚Äî D√©ployer Traefik via ArgoCD

**Depuis votre poste de travail.**

```bash
cd ~/k3s-gitops
git add infrastructure/traefik/ argocd-apps/traefik.yaml
git commit -m "Add Traefik Ingress Controller with middlewares and ServiceMonitor"
git push origin main

kubectl apply -f argocd-apps/traefik.yaml

# Suivre la synchronisation (2-3 minutes)
kubectl get application -n argocd traefik -w
```

### V√©rification

```bash
# Pods Traefik (3 pods, un par n≈ìud)
kubectl get pods -n traefik -o wide

# DaemonSet
kubectl get daemonset -n traefik
# DESIRED=3, CURRENT=3, READY=3

# Middlewares
kubectl get middleware -n traefik
# Les 6 middlewares doivent appara√Ætre
```

### Test d‚Äôacc√®s

```bash
# HTTP ‚Üí doit rediriger vers HTTPS (308)
curl -I http://<IP_PUBLIQUE_NOEUD1> 
# R√©sultat attendu : HTTP/1.1 308 Permanent Redirect

# HTTPS ‚Üí 404 attendu car aucun IngressRoute ne matche encore
curl -I -k https://<IP_PUBLIQUE_NOEUD1>
# R√©sultat attendu : 404 page not found

# Tester sur les 3 n≈ìuds
curl -k https://<IP_PUBLIQUE_NOEUD2>
curl -k https://<IP_PUBLIQUE_NOEUD3>
```

### Middlewares Traefik disponibles

Tous les middlewares sont dans le namespace `traefik`. Pour les utiliser depuis un autre namespace, pr√©fixer avec `traefik-` et suffixer avec `@kubernetescrd`.

| Middleware | Usage | Description |
| --- | --- | --- |
| `default-security-chain` | Recommand√© pour toutes les apps | Headers s√©curit√© + compression + rate limit 100/s |
| `security-headers` | Headers uniquement | HSTS, X-Frame-Options, CSP, etc. |
| `compression` | Performance | Compression gzip |
| `rate-limit` | Protection DDoS basique | 100 req/s, burst 200 |
| `rate-limit-api` | APIs | 20 req/s, burst 50 |
| `retry` | R√©silience | 3 tentatives avec backoff |

---

## Phase 6 : D√©ploiement de cert-manager via GitOps

### √âtape 16 ‚Äî Pr√©parer le d√©ploiement de cert-manager

cert-manager automatise l‚Äô√©mission et le renouvellement des certificats TLS via Let‚Äôs Encrypt. Il remplace les certificats auto-sign√©s de Traefik par des certificats reconnus par les navigateurs. Les challenges HTTP-01 passent par Traefik pour prouver la propri√©t√© du domaine, et les certificats sont stock√©s dans des Secrets Kubernetes.

**Pr√©requis :** Traefik op√©rationnel (Phase 5), un nom de domaine avec des enregistrements DNS A pointant vers les IP publiques des n≈ìuds, les ports 80 et 443 ouverts (d√©j√† fait √† l‚Äô√©tape 14).

> Les challenges HTTP-01 n√©cessitent que Let‚Äôs Encrypt puisse joindre votre domaine sur le port 80. Traefik redirige HTTP ‚Üí HTTPS **sauf** pour les chemins `/.well-known/acme-challenge/` qui sont intercept√©s par cert-manager pendant la validation.
> 

### Configurer les enregistrements DNS

Cr√©er des enregistrements DNS A chez votre registrar :

```
# Round-Robin DNS sur les 3 n≈ìuds
app.votredomaine.com    A    <IP_PUBLIQUE_NOEUD1>
app.votredomaine.com    A    <IP_PUBLIQUE_NOEUD2>
app.votredomaine.com    A    <IP_PUBLIQUE_NOEUD3>

# Ou avec un wildcard
*.votredomaine.com      A    <IP_PUBLIQUE_NOEUD1>
*.votredomaine.com      A    <IP_PUBLIQUE_NOEUD2>
*.votredomaine.com      A    <IP_PUBLIQUE_NOEUD3>
```

V√©rifier la propagation :

```bash
dig +short app.votredomaine.com
# R√©sultat attendu : les 3 IP publiques
```

> La propagation DNS peut prendre quelques minutes.
> 

### Cr√©er le values.yaml cert-manager

```bash
mkdir -p infrastructure/cert-manager

cat > infrastructure/cert-manager/values.yaml <<'EOF'
# Configuration cert-manager pour K3s 3 masters

crds:
  enabled: true
  keep: true

replicaCount: 2

webhook:
  replicaCount: 2
  tolerations:
    - key: node-role.kubernetes.io/control-plane
      operator: Exists
      effect: NoSchedule
    - key: node-role.kubernetes.io/master
      operator: Exists
      effect: NoSchedule
  resources:
    requests:
      cpu: 50m
      memory: 64Mi
    limits:
      cpu: 200m
      memory: 256Mi

cainjector:
  replicaCount: 1
  tolerations:
    - key: node-role.kubernetes.io/control-plane
      operator: Exists
      effect: NoSchedule
    - key: node-role.kubernetes.io/master
      operator: Exists
      effect: NoSchedule
  resources:
    requests:
      cpu: 50m
      memory: 64Mi
    limits:
      cpu: 200m
      memory: 256Mi

tolerations:
  - key: node-role.kubernetes.io/control-plane
    operator: Exists
    effect: NoSchedule
  - key: node-role.kubernetes.io/master
    operator: Exists
    effect: NoSchedule

resources:
  requests:
    cpu: 50m
    memory: 64Mi
  limits:
    cpu: 200m
    memory: 256Mi

prometheus:
  enabled: false
  servicemonitor:
    enabled: false

global:
  logLevel: 2
EOF
```

### Cr√©er les ClusterIssuers Let‚Äôs Encrypt

```bash
cat > infrastructure/cert-manager/cluster-issuers.yaml <<'EOF'
---
apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
  name: letsencrypt-staging
spec:
  acme:
    email: votre-email@example.com  # ‚ö†Ô∏è REMPLACER
    server: https://acme-staging-v02.api.letsencrypt.org/directory
    privateKeySecretRef:
      name: letsencrypt-staging-account-key
    solvers:
      - http01:
          ingress:
            class: traefik
---
apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
  name: letsencrypt-prod
spec:
  acme:
    email: votre-email@example.com  # ‚ö†Ô∏è REMPLACER
    server: https://acme-v02.api.letsencrypt.org/directory
    privateKeySecretRef:
      name: letsencrypt-prod-account-key
    solvers:
      - http01:
          ingress:
            class: traefik
EOF
```

> Remplacer  `votre-email@example.com` par une addresse mail l√©gitime.
> 

```bash
sed -i 's|votre-email@example.com|votre-VRAI-email@example.com|g' infrastructure/cert-manager/cluster-issuers.yaml
```

### Cr√©er le kustomization.yaml

```bash
cat > infrastructure/cert-manager/kustomization.yaml <<'EOF'
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
namespace: cert-manager
resources:
  - cluster-issuers.yaml
EOF
```

### Cr√©er l‚ÄôApplication ArgoCD pour cert-manager

```bash
cat > argocd-apps/cert-manager.yaml <<EOF
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: cert-manager
  namespace: argocd
  finalizers:
    - resources-finalizer.argocd.argoproj.io
spec:
  project: default

  sources:
    - repoURL: 'https://charts.jetstack.io'
      targetRevision: v1.17.2
      chart: cert-manager
      helm:
        releaseName: cert-manager
        valueFiles:
          - '\$values/infrastructure/cert-manager/values.yaml'

    - repoURL: '$GIT_URL'
      targetRevision: main
      ref: values

    - repoURL: '$GIT_URL'
      targetRevision: main
      path: infrastructure/cert-manager

  destination:
    server: 'https://kubernetes.default.svc'
    namespace: cert-manager

  syncPolicy:
    automated:
      prune: true
      selfHeal: true
      allowEmpty: false
    syncOptions:
      - CreateNamespace=true
      - ServerSideApply=true
    retry:
      limit: 5
      backoff:
        duration: 5s
        factor: 2
        maxDuration: 3m
EOF
```

---

### √âtape 17 ‚Äî D√©ployer cert-manager via ArgoCD

**Depuis votre poste de travail.**

```bash
cd ~/k3s-gitops
git add infrastructure/cert-manager/ argocd-apps/cert-manager.yaml
git commit -m "Add cert-manager with Let's Encrypt ClusterIssuers and ServiceMonitor"
git push origin main

kubectl apply -f argocd-apps/cert-manager.yaml

kubectl get application -n argocd cert-manager -w
```

> Les ClusterIssuers peuvent temporairement √©chouer pendant l‚Äôinstallation des CRDs. ArgoCD retente automatiquement gr√¢ce √† la politique de retry.
> 

### V√©rification

```bash
kubectl get pods -n cert-manager
kubectl get deployments -n cert-manager

kubectl get crds | grep cert-manager
# certificaterequests, certificates, challenges, clusterissuers, issuers, orders

kubectl get clusterissuers
# Les deux issuers doivent √™tre READY=True
```

---

### √âtape 18 ‚Äî Exposer ArgoCD via Ingress HTTPS avec cert-manager

Cette √©tape expose ArgoCD via un Ingress Traefik avec un certificat Let‚Äôs Encrypt. Elle remplace l‚Äôexposition temporaire via NodePort (√©tape 8) et valide l‚Äôint√©gration cert-manager + Traefik.

> **D√©pendance circulaire :** ArgoCD va g√©rer sa propre exposition ‚Äî c‚Äôest un pattern courant appel√© ‚Äúself-managing ArgoCD‚Äù. Pendant la transition, ArgoCD reste accessible via le NodePort temporaire pour synchroniser les nouveaux manifestes.
> 

**Depuis votre poste de travail.**

### V√©rifier le DNS

```bash
dig +short argocd.votredomaine.com
# R√©sultat attendu : les 3 IP publiques
```

### Configurer ArgoCD pour un environnement sans LoadBalancer

**1. Passer ArgoCD en mode insecure (TLS termin√© par Traefik)**

Sans cette option, ArgoCD sert du HTTPS sur son port interne. Traefik recevrait du trafic chiffr√© en backend, provoquant des erreurs 502 ou des boucles de redirection. En mode insecure, ArgoCD sert du HTTP en interne et Traefik g√®re le TLS c√¥t√© client.

```bash
kubectl -n argocd patch configmap argocd-cmd-params-cm \
  --type merge \
  -p '{"data":{"server.insecure":"true"}}'
```

**2. Corriger le health check des Ingress**

Par d√©faut, ArgoCD consid√®re un Ingress comme `Progressing` tant que `status.loadBalancer` n‚Äôest pas renseign√©. Dans notre setup avec `hostPort`, ce champ reste toujours vide. Sans cette correction, toutes les applications utilisant un Ingress resteraient bloqu√©es en `Progressing`.

```bash
kubectl -n argocd patch configmap argocd-cm \
  --type merge \
  -p '{
    "data": {
      "resource.customizations.health.networking.k8s.io_Ingress": "hs = {}\nhs.status = \"Healthy\"\nhs.message = \"Ingress is healthy (hostPort mode, no LoadBalancer)\"\nreturn hs"
    }
  }'
```

**3. Red√©marrer le serveur**

```bash
kubectl -n argocd rollout restart deployment argocd-server
kubectl -n argocd rollout status deployment argocd-server
# R√©sultat attendu : deployment "argocd-server" successfully rolled out
```

### Cr√©er les manifestes d‚Äôexposition ArgoCD

```bash
mkdir -p infrastructure/argocd

cat > infrastructure/argocd/ingress.yaml <<'EOF'
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: argocd-server
  namespace: argocd
  annotations:
    traefik.ingress.kubernetes.io/router.entrypoints: websecure
    traefik.ingress.kubernetes.io/router.tls: "true"
    cert-manager.io/cluster-issuer: letsencrypt-prod
spec:
  tls:
    - hosts:
        - argocd.k0li.fr # ‚ö†Ô∏è REMPLACER
      secretName: argocd-server-tls
  rules:
    - host: argocd.k0li.fr  # ‚ö†Ô∏è REMPLACER
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: argocd-server
                port:
                  number: 80
EOF
```

**Choix de conception :**

| Choix | Raison |
| --- | --- |
| `port: 80` | ArgoCD est en mode insecure, il sert du HTTP en interne |
| Pas de middleware `default-security-chain` | ArgoCD g√®re ses propres headers et n√©cessite WebSocket pour le streaming des logs |

### Cr√©er le kustomization.yaml et l‚ÄôApplication ArgoCD

```bash
cat > infrastructure/argocd/kustomization.yaml <<'EOF'
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
namespace: argocd
resources:
  - ingress.yaml
EOF

cat > argocd-apps/argocd-ingress.yaml <<EOF
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: argocd-ingress
  namespace: argocd
  finalizers:
    - resources-finalizer.argocd.argoproj.io
spec:
  project: default

  sources:
    - repoURL: '$GIT_URL'
      targetRevision: main
      path: infrastructure/argocd

  destination:
    server: 'https://kubernetes.default.svc'
    namespace: argocd

  syncPolicy:
    automated:
      prune: true
      selfHeal: true
      allowEmpty: false
    syncOptions:
      - ServerSideApply=true
    retry:
      limit: 5
      backoff:
        duration: 5s
        factor: 2
        maxDuration: 3m
EOF
```

> Remplacer `argocd.votredomaine.com` par votre domaine r√©el dans `infrastructure/argocd/ingress.yaml`
> 

```bash
sed -i 's|argocd.votredomaine.com|argocd.votreVRAIdomaine.com|g' infrastructure/argocd/ingress.yaml
```

### Pousser et d√©ployer

```bash
cd ~/k3s-gitops
git add infrastructure/argocd/ argocd-apps/argocd-ingress.yaml
git commit -m "Expose ArgoCD via Ingress with cert-manager TLS (staging)"
git push origin main

kubectl apply -f argocd-apps/argocd-ingress.yaml

kubectl get application -n argocd argocd-ingress -w
```

### V√©rifier le certificat

```bash
kubectl get certificate -n argocd -w
# Attendre que READY passe √† True (1-3 minutes)

curl -k -v https://argocd.votredomaine.com 2>&1 | grep -E "subject:|issuer:|< HTTP"
# R√©sultat attendu : subject CN=argocd.votredomaine.com, issuer C=US, HTTP/2 200
```

V√©rifier que l‚Äôinterface ArgoCD s‚Äôaffiche dans le navigateur

---

### Supprimer l‚Äôacc√®s NodePort temporaire

```bash
kubectl patch svc argocd-server -n argocd -p '{"spec": {"type": "ClusterIP"}}'
```

Sur le n≈ìud 1, supprimer la r√®gle firewall temporaire :

```bash
sudo ufw status numbered
# Identifier la r√®gle "ArgoCD NodePort temporaire"
sudo ufw delete <num√©ro>
```

---

## Phase 7 : D√©ploiement de Kyverno via GitOps

---

## √âtape 20 ‚Äî Pr√©parer le d√©ploiement de Kyverno

Kyverno est un contr√¥leur d'admission Kubernetes natif. Il permet de d√©finir des policies d√©claratives en YAML, versionnables dans Git, pour valider, muter et g√©n√©rer des ressources. Il inclut des rapports d'audit int√©gr√©s via PolicyReports.

**Architecture :** Kyverno est d√©ploy√© en haute disponibilit√© (3 r√©plicas). Les policies applicatives sont en mode `Enforce` : les ressources non conformes sont rejet√©es √† l'admission. Les policies sur les namespaces infra sont en mode `Audit` : les violations sont remont√©es dans les PolicyReports sans blocage. Les namespaces syst√®me Kubernetes sont exclus des webhooks et du scan en arri√®re-plan.

**Strat√©gie d'application des policies :**

| P√©rim√®tre | Namespaces | Mode | Effet |
| --- | --- | --- | --- |
| Syst√®me K8s | `kube-system`, `kube-public`, `kube-node-lease` | Exclus du webhook et du background scan | Kyverno n'intervient pas du tout |
| Infra cluster | `argocd`, `traefik`, `longhorn-system`, `cert-manager`, `kyverno`, `monitoring`, `loki` | `Audit` | Violations remont√©es dans les PolicyReports, rien bloqu√© |
| Applicatif | Tous les autres namespaces | `Enforce` | Blocage en cas de non-conformit√© |

> **Pourquoi deux niveaux d'exclusion pour les namespaces syst√®me ?** Le `webhooks.namespaceSelector` filtre les requ√™tes d'admission entrantes. Mais le `backgroundController` scanne les ressources *existantes* via un chemin s√©par√©, contournant cette exclusion. La section `backgroundScan` dans le values.yaml corrige ce comportement.
> 

> **Pourquoi les namespaces infra ne sont-ils plus exclus du webhook ?** Dans la configuration pr√©c√©dente, ils √©taient exclus, ce qui emp√™chait tout audit. Ils sont maintenant couverts par des r√®gles `Audit` d√©di√©es, permettant de remonter des √©v√©nements sans rien bloquer.
> 

**Depuis votre poste de travail.**

### Cr√©er le values.yaml Kyverno

```bash
mkdir -p infrastructure/kyverno

cat > infrastructure/kyverno/values.yaml <<'EOF'
# Configuration Kyverno pour K3s 3 masters

admissionController:
  replicas: 3

  tolerations:
    - key: node-role.kubernetes.io/control-plane
      operator: Exists
      effect: NoSchedule
    - key: node-role.kubernetes.io/master
      operator: Exists
      effect: NoSchedule

  antiAffinity:
    enabled: true

  resources:
    requests:
      cpu: 100m
      memory: 256Mi
    limits:
      cpu: 500m
      memory: 512Mi

  serviceMonitor:
    enabled: false

backgroundController:
  replicas: 2

  tolerations:
    - key: node-role.kubernetes.io/control-plane
      operator: Exists
      effect: NoSchedule
    - key: node-role.kubernetes.io/master
      operator: Exists
      effect: NoSchedule

  resources:
    requests:
      cpu: 50m
      memory: 128Mi
    limits:
      cpu: 200m
      memory: 256Mi

  serviceMonitor:
    enabled: false

cleanupController:
  replicas: 1

  tolerations:
    - key: node-role.kubernetes.io/control-plane
      operator: Exists
      effect: NoSchedule
    - key: node-role.kubernetes.io/master
      operator: Exists
      effect: NoSchedule

  resources:
    requests:
      cpu: 50m
      memory: 64Mi
    limits:
      cpu: 200m
      memory: 256Mi

  serviceMonitor:
    enabled: false

reportsController:
  replicas: 1

  tolerations:
    - key: node-role.kubernetes.io/control-plane
      operator: Exists
      effect: NoSchedule
    - key: node-role.kubernetes.io/master
      operator: Exists
      effect: NoSchedule

  resources:
    requests:
      cpu: 50m
      memory: 128Mi
    limits:
      cpu: 200m
      memory: 256Mi

  serviceMonitor:
    enabled: false

config:
  # Exclut uniquement les namespaces syst√®me Kubernetes du webhook d'admission.
  # Les namespaces infra sont g√©r√©s via des r√®gles Audit dans les ClusterPolicies.
  webhooks:
    namespaceSelector:
      matchExpressions:
        - key: kubernetes.io/metadata.name
          operator: NotIn
          values:
            - kube-system
            - kube-public
            - kube-node-lease

  # Exclut les namespaces syst√®me du scan en arri√®re-plan (backgroundController).
  # Sans cette section, le backgroundController scanne les ressources existantes
  # dans tous les namespaces, contournant l'exclusion du webhook.
  backgroundScan:
    backgroundScanWorkers: 2
    backgroundScanInterval: 1h
    skipResourceFilters:
      - apiVersion: "*"
        kind: Pod
        selector:
          namespaceSelector:
            matchExpressions:
              - key: kubernetes.io/metadata.name
                operator: In
                values:
                  - kube-system
                  - kube-public
                  - kube-node-lease

metricsConfig:
  metricsExposure:
    enabled: false
EOF
```

### Cr√©er les policies

**Policy 1 : Resources requests et limits obligatoires**

```bash
cat > infrastructure/kyverno/policy-require-resources.yaml <<'EOF'
apiVersion: kyverno.io/v1
kind: ClusterPolicy
metadata:
  name: require-resources
  annotations:
    policies.kyverno.io/title: Require Resource Requests and Limits
    policies.kyverno.io/description: >-
      Enforce sur les namespaces applicatifs : tous les containers doivent d√©finir
      requests et limits CPU/m√©moire. Audit sur les namespaces infra.
    policies.kyverno.io/severity: high
    policies.kyverno.io/category: Best Practices
spec:
  rules:

    # --- Enforce : namespaces applicatifs ---
    - name: require-resources-enforce
      match:
        any:
          - resources:
              kinds:
                - Pod
      exclude:
        any:
          - resources:
              namespaces:
                - kube-system
                - kube-public
                - kube-node-lease
                - argocd
                - traefik
                - longhorn-system
                - cert-manager
                - kyverno
                - monitoring
                - loki
      validate:
        failureAction: Enforce
        message: >-
          [ENFORCE] Tous les containers doivent d√©finir resources.requests et
          resources.limits pour cpu et memory.
        foreach:
          - list: "request.object.spec.containers"
            deny:
              conditions:
                any:
                  - key: "{{ element.resources.requests.cpu || '' }}"
                    operator: Equals
                    value: ""
                  - key: "{{ element.resources.requests.memory || '' }}"
                    operator: Equals
                    value: ""
                  - key: "{{ element.resources.limits.cpu || '' }}"
                    operator: Equals
                    value: ""
                  - key: "{{ element.resources.limits.memory || '' }}"
                    operator: Equals
                    value: ""

    - name: require-resources-init-enforce
      match:
        any:
          - resources:
              kinds:
                - Pod
      exclude:
        any:
          - resources:
              namespaces:
                - kube-system
                - kube-public
                - kube-node-lease
                - argocd
                - traefik
                - longhorn-system
                - cert-manager
                - kyverno
                - monitoring
                - loki
      preconditions:
        all:
          - key: "{{ request.object.spec.initContainers[] || `[]` | length(@) }}"
            operator: GreaterThanOrEquals
            value: 1
      validate:
        failureAction: Enforce
        message: >-
          [ENFORCE] Tous les initContainers doivent d√©finir resources.requests et
          resources.limits pour cpu et memory.
        foreach:
          - list: "request.object.spec.initContainers"
            deny:
              conditions:
                any:
                  - key: "{{ element.resources.requests.cpu || '' }}"
                    operator: Equals
                    value: ""
                  - key: "{{ element.resources.requests.memory || '' }}"
                    operator: Equals
                    value: ""
                  - key: "{{ element.resources.limits.cpu || '' }}"
                    operator: Equals
                    value: ""
                  - key: "{{ element.resources.limits.memory || '' }}"
                    operator: Equals
                    value: ""

    # --- Audit : namespaces infra ---
    - name: require-resources-audit-infra
      match:
        any:
          - resources:
              kinds:
                - Pod
              namespaces:
                - argocd
                - traefik
                - longhorn-system
                - cert-manager
                - kyverno
                - monitoring
                - loki
      validate:
        failureAction: Audit
        message: >-
          [AUDIT] Container sans resources.requests/limits d√©tect√© dans un namespace infra.
          Corrigez pour pr√©parer un √©ventuel passage en Enforce.
        foreach:
          - list: "request.object.spec.containers"
            deny:
              conditions:
                any:
                  - key: "{{ element.resources.requests.cpu || '' }}"
                    operator: Equals
                    value: ""
                  - key: "{{ element.resources.requests.memory || '' }}"
                    operator: Equals
                    value: ""
                  - key: "{{ element.resources.limits.cpu || '' }}"
                    operator: Equals
                    value: ""
                  - key: "{{ element.resources.limits.memory || '' }}"
                    operator: Equals
                    value: ""
EOF
```

**Policy 2 : SecurityContext renforc√©**

```bash
cat > infrastructure/kyverno/policy-require-security-context.yaml <<'EOF'
apiVersion: kyverno.io/v1
kind: ClusterPolicy
metadata:
  name: require-security-context
  annotations:
    policies.kyverno.io/title: Require Strict SecurityContext
    policies.kyverno.io/description: >-
      Enforce sur les namespaces applicatifs : runAsNonRoot (pod), runAsUser != 0,
      allowPrivilegeEscalation: false, capabilities.drop: [ALL] (container).
      Audit sur les namespaces infra.
    policies.kyverno.io/severity: high
    policies.kyverno.io/category: Pod Security
spec:
  rules:

    # --- Enforce : namespaces applicatifs ---
    - name: require-pod-run-as-non-root-enforce
      match:
        any:
          - resources:
              kinds:
                - Pod
      exclude:
        any:
          - resources:
              namespaces:
                - kube-system
                - kube-public
                - kube-node-lease
                - argocd
                - traefik
                - longhorn-system
                - cert-manager
                - kyverno
                - monitoring
                - loki
      validate:
        failureAction: Enforce
        message: "[ENFORCE] spec.securityContext.runAsNonRoot doit √™tre d√©fini √† true."
        pattern:
          spec:
            securityContext:
              runAsNonRoot: true

    - name: require-container-secctx-enforce
      match:
        any:
          - resources:
              kinds:
                - Pod
      exclude:
        any:
          - resources:
              namespaces:
                - kube-system
                - kube-public
                - kube-node-lease
                - argocd
                - traefik
                - longhorn-system
                - cert-manager
                - kyverno
                - monitoring
                - loki
      validate:
        failureAction: Enforce
        message: >-
          [ENFORCE] Tous les containers doivent d√©finir : runAsUser > 0,
          allowPrivilegeEscalation: false, capabilities.drop contenant ALL.
        foreach:
          - list: "request.object.spec.containers"
            deny:
              conditions:
                any:
                  - key: "{{ element.securityContext.runAsUser || `0` }}"
                    operator: Equals
                    value: 0
                  - key: "{{ element.securityContext.allowPrivilegeEscalation || true }}"
                    operator: NotEquals
                    value: false
                  - key: "ALL"
                    operator: AnyNotIn
                    value: "{{ element.securityContext.capabilities.drop || `[]` }}"

    - name: require-init-secctx-enforce
      match:
        any:
          - resources:
              kinds:
                - Pod
      exclude:
        any:
          - resources:
              namespaces:
                - kube-system
                - kube-public
                - kube-node-lease
                - argocd
                - traefik
                - longhorn-system
                - cert-manager
                - kyverno
                - monitoring
                - loki
      preconditions:
        all:
          - key: "{{ request.object.spec.initContainers[] || `[]` | length(@) }}"
            operator: GreaterThanOrEquals
            value: 1
      validate:
        failureAction: Enforce
        message: >-
          [ENFORCE] Tous les initContainers doivent d√©finir : runAsUser > 0,
          allowPrivilegeEscalation: false, capabilities.drop contenant ALL.
        foreach:
          - list: "request.object.spec.initContainers"
            deny:
              conditions:
                any:
                  - key: "{{ element.securityContext.runAsUser || `0` }}"
                    operator: Equals
                    value: 0
                  - key: "{{ element.securityContext.allowPrivilegeEscalation || true }}"
                    operator: NotEquals
                    value: false
                  - key: "ALL"
                    operator: AnyNotIn
                    value: "{{ element.securityContext.capabilities.drop || `[]` }}"

    # --- Audit : namespaces infra ---
    - name: require-pod-run-as-non-root-audit
      match:
        any:
          - resources:
              kinds:
                - Pod
              namespaces:
                - argocd
                - traefik
                - longhorn-system
                - cert-manager
                - kyverno
                - monitoring
                - loki
      validate:
        failureAction: Audit
        message: >-
          [AUDIT] spec.securityContext.runAsNonRoot n'est pas d√©fini √† true
          dans un namespace infra.
        pattern:
          spec:
            securityContext:
              runAsNonRoot: true

    - name: require-container-secctx-audit
      match:
        any:
          - resources:
              kinds:
                - Pod
              namespaces:
                - argocd
                - traefik
                - longhorn-system
                - cert-manager
                - kyverno
                - monitoring
                - loki
      validate:
        failureAction: Audit
        message: >-
          [AUDIT] Container sans SecurityContext strict dans un namespace infra
          (runAsUser, allowPrivilegeEscalation, capabilities.drop).
        foreach:
          - list: "request.object.spec.containers"
            deny:
              conditions:
                any:
                  - key: "{{ element.securityContext.runAsUser || `0` }}"
                    operator: Equals
                    value: 0
                  - key: "{{ element.securityContext.allowPrivilegeEscalation || true }}"
                    operator: NotEquals
                    value: false
                  - key: "ALL"
                    operator: AnyNotIn
                    value: "{{ element.securityContext.capabilities.drop || `[]` }}"
EOF
```

**Policy 3 : Interdire les containers privil√©gi√©s**

```bash
cat > infrastructure/kyverno/policy-disallow-privileged.yaml <<'EOF'
apiVersion: kyverno.io/v1
kind: ClusterPolicy
metadata:
  name: disallow-privileged-containers
  annotations:
    policies.kyverno.io/title: Disallow Privileged Containers
    policies.kyverno.io/description: >-
      Enforce sur les namespaces applicatifs : interdit securityContext.privileged=true.
      Audit sur les namespaces infra.
    policies.kyverno.io/severity: high
    policies.kyverno.io/category: Pod Security
spec:
  rules:

    # --- Enforce : namespaces applicatifs ---
    - name: disallow-privileged-enforce
      match:
        any:
          - resources:
              kinds:
                - Pod
      exclude:
        any:
          - resources:
              namespaces:
                - kube-system
                - kube-public
                - kube-node-lease
                - argocd
                - traefik
                - longhorn-system
                - cert-manager
                - kyverno
                - monitoring
                - loki
      validate:
        failureAction: Enforce
        message: "[ENFORCE] Les containers privil√©gi√©s sont interdits."
        pattern:
          spec:
            containers:
              - =(securityContext):
                  =(privileged): false
            =(initContainers):
              - =(securityContext):
                  =(privileged): false
            =(ephemeralContainers):
              - =(securityContext):
                  =(privileged): false

    # --- Audit : namespaces infra ---
    # Note : certains composants infra utilisent privileged l√©gitimement (ex: Longhorn).
    # L'audit permet de les identifier sans les bloquer.
    - name: disallow-privileged-audit
      match:
        any:
          - resources:
              kinds:
                - Pod
              namespaces:
                - argocd
                - traefik
                - longhorn-system
                - cert-manager
                - kyverno
                - monitoring
                - loki
      validate:
        failureAction: Audit
        message: "[AUDIT] Container privil√©gi√© d√©tect√© dans un namespace infra."
        pattern:
          spec:
            containers:
              - =(securityContext):
                  =(privileged): false
            =(initContainers):
              - =(securityContext):
                  =(privileged): false
            =(ephemeralContainers):
              - =(securityContext):
                  =(privileged): false
EOF
```

**Policy 4 : Interdire les namespaces h√¥te**

```bash
cat > infrastructure/kyverno/policy-disallow-host-namespaces.yaml <<'EOF'
apiVersion: kyverno.io/v1
kind: ClusterPolicy
metadata:
  name: disallow-host-namespaces
  annotations:
    policies.kyverno.io/title: Disallow Host Namespaces
    policies.kyverno.io/description: >-
      Enforce sur les namespaces applicatifs : interdit hostPID, hostIPC, hostNetwork.
      Audit sur les namespaces infra (node-exporter utilise hostNetwork l√©gitimement).
    policies.kyverno.io/severity: high
    policies.kyverno.io/category: Pod Security
spec:
  rules:

    # --- Enforce : namespaces applicatifs ---
    - name: disallow-host-namespaces-enforce
      match:
        any:
          - resources:
              kinds:
                - Pod
      exclude:
        any:
          - resources:
              namespaces:
                - kube-system
                - kube-public
                - kube-node-lease
                - argocd
                - traefik
                - longhorn-system
                - cert-manager
                - kyverno
                - monitoring
                - loki
      validate:
        failureAction: Enforce
        message: "[ENFORCE] hostPID, hostIPC et hostNetwork sont interdits."
        pattern:
          spec:
            =(hostPID): false
            =(hostIPC): false
            =(hostNetwork): false

    # --- Audit : namespaces infra ---
    # node-exporter (monitoring) utilise hostNetwork et hostPID l√©gitimement.
    # L'audit remonte l'information sans bloquer.
    - name: disallow-host-namespaces-audit
      match:
        any:
          - resources:
              kinds:
                - Pod
              namespaces:
                - argocd
                - traefik
                - longhorn-system
                - cert-manager
                - kyverno
                - monitoring
                - loki
      validate:
        failureAction: Audit
        message: >-
          [AUDIT] Utilisation de hostPID/hostIPC/hostNetwork d√©tect√©e dans un namespace infra.
          V√©rifiez si c'est intentionnel (ex: node-exporter).
        pattern:
          spec:
            =(hostPID): false
            =(hostIPC): false
            =(hostNetwork): false
EOF
```

**Policy 5 : Taille des volumes obligatoire**

```bash
cat > infrastructure/kyverno/policy-require-storage-size.yaml <<'EOF'
apiVersion: kyverno.io/v1
kind: ClusterPolicy
metadata:
  name: require-storage-size
  annotations:
    policies.kyverno.io/title: Require Storage Size on PVC and emptyDir
    policies.kyverno.io/description: >-
      Enforce sur les namespaces applicatifs : emptyDir doit avoir un sizeLimit,
      les PVC doivent d√©finir requests.storage. Audit sur les namespaces infra.
    policies.kyverno.io/severity: high
    policies.kyverno.io/category: Best Practices
spec:
  rules:

    # --- Enforce : namespaces applicatifs ---
    - name: require-emptydir-size-enforce
      match:
        any:
          - resources:
              kinds:
                - Pod
      exclude:
        any:
          - resources:
              namespaces:
                - kube-system
                - kube-public
                - kube-node-lease
                - argocd
                - traefik
                - longhorn-system
                - cert-manager
                - kyverno
                - monitoring
                - loki
      preconditions:
        all:
          - key: "{{ request.object.spec.volumes[?emptyDir] || `[]` | length(@) }}"
            operator: GreaterThanOrEquals
            value: 1
      validate:
        failureAction: Enforce
        message: "[ENFORCE] Tous les volumes emptyDir doivent d√©finir un sizeLimit."
        foreach:
          - list: "request.object.spec.volumes[?emptyDir]"
            deny:
              conditions:
                any:
                  - key: "{{ element.emptyDir.sizeLimit || '' }}"
                    operator: Equals
                    value: ""

    - name: require-pvc-size-enforce
      match:
        any:
          - resources:
              kinds:
                - PersistentVolumeClaim
      exclude:
        any:
          - resources:
              namespaces:
                - kube-system
                - kube-public
                - kube-node-lease
                - argocd
                - traefik
                - longhorn-system
                - cert-manager
                - kyverno
                - monitoring
                - loki
      validate:
        failureAction: Enforce
        message: "[ENFORCE] Les PVC doivent d√©finir spec.resources.requests.storage."
        pattern:
          spec:
            resources:
              requests:
                storage: "?*"

    # --- Audit : namespaces infra ---
    - name: require-emptydir-size-audit
      match:
        any:
          - resources:
              kinds:
                - Pod
              namespaces:
                - argocd
                - traefik
                - longhorn-system
                - cert-manager
                - kyverno
                - monitoring
                - loki
      preconditions:
        all:
          - key: "{{ request.object.spec.volumes[?emptyDir] || `[]` | length(@) }}"
            operator: GreaterThanOrEquals
            value: 1
      validate:
        failureAction: Audit
        message: "[AUDIT] Volume emptyDir sans sizeLimit dans un namespace infra."
        foreach:
          - list: "request.object.spec.volumes[?emptyDir]"
            deny:
              conditions:
                any:
                  - key: "{{ element.emptyDir.sizeLimit || '' }}"
                    operator: Equals
                    value: ""
EOF
```

### Cr√©er le kustomization.yaml Kyverno

> Inchang√© par rapport √† la version pr√©c√©dente.
> 

```bash
cat > infrastructure/kyverno/kustomization.yaml <<'EOF'
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - policy-require-resources.yaml
  - policy-require-security-context.yaml
  - policy-disallow-privileged.yaml
  - policy-disallow-host-namespaces.yaml
  - policy-require-storage-size.yaml
EOF
```

### Cr√©er l'Application ArgoCD pour Kyverno

> Inchang√©e par rapport √† la version pr√©c√©dente.
> 

```bash
cat > argocd-apps/kyverno.yaml <<EOF
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: kyverno
  namespace: argocd
  finalizers:
    - resources-finalizer.argocd.argoproj.io
spec:
  project: default

  sources:
    - repoURL: 'https://kyverno.github.io/kyverno'
      targetRevision: 3.7.0
      chart: kyverno
      helm:
        releaseName: kyverno
        valueFiles:
          - '\$values/infrastructure/kyverno/values.yaml'

    - repoURL: '$GIT_URL'
      targetRevision: main
      ref: values

    - repoURL: '$GIT_URL'
      targetRevision: main
      path: infrastructure/kyverno

  destination:
    server: 'https://kubernetes.default.svc'
    namespace: kyverno

  syncPolicy:
    automated:
      prune: true
      selfHeal: true
      allowEmpty: false
    syncOptions:
      - CreateNamespace=true
      - ServerSideApply=true
    retry:
      limit: 5
      backoff:
        duration: 5s
        factor: 2
        maxDuration: 3m
EOF
```

## √âtape 21 ‚Äî D√©ployer Kyverno via ArgoCD

**Depuis votre poste de travail.**

```bash
cd ~/k3s-gitops
git add infrastructure/kyverno/ argocd-apps/kyverno.yaml
git commit -m "Kyverno: Enforce apps, Audit infra, fix background scan exclusions"
git push origin main

kubectl apply -f argocd-apps/kyverno.yaml

kubectl get application -n argocd kyverno -w
```

> Les ClusterPolicies peuvent temporairement √©chouer pendant l'installation des CRDs. ArgoCD retente automatiquement.
> 

### V√©rification

```bash
kubectl get pods -n kyverno
# admissionController (x3), backgroundController (x2), cleanupController (x1), reportsController (x1)

kubectl get clusterpolicies
# Les 5 policies doivent √™tre READY=True
```

---

## √âtape 22 ‚Äî Tester les policies Kyverno

**Depuis votre poste de travail.**

### Test 1 : Pod sans resources dans un namespace applicatif (doit √™tre rejet√©)

```bash
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: test-no-resources
  namespace: default
spec:
  securityContext:
    runAsNonRoot: true
  containers:
  - name: nginx
    image: nginx:alpine
    securityContext:
      runAsUser: 1000
      allowPrivilegeEscalation: false
      capabilities:
        drop: ["ALL"]
EOF
# R√©sultat attendu : [ENFORCE] denied the request ‚Äî resources manquantes
```

### Test 2 : Pod sans SecurityContext dans un namespace applicatif (doit √™tre rejet√©)

```bash
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: test-no-secctx
  namespace: default
spec:
  containers:
  - name: nginx
    image: nginx:alpine
    resources:
      requests:
        cpu: 50m
        memory: 64Mi
      limits:
        cpu: 100m
        memory: 128Mi
EOF
# R√©sultat attendu : [ENFORCE] denied the request ‚Äî securityContext manquant
```

### Test 3 : Pod conforme dans un namespace applicatif (doit √™tre accept√©)

```bash
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: test-compliant
  namespace: default
spec:
  securityContext:
    runAsNonRoot: true
    fsGroup: 2000
  containers:
  - name: nginx
    image: nginx:alpine
    resources:
      requests:
        cpu: 50m
        memory: 64Mi
      limits:
        cpu: 100m
        memory: 128Mi
    securityContext:
      runAsUser: 1000
      allowPrivilegeEscalation: false
      capabilities:
        drop: ["ALL"]
    volumeMounts:
      - name: cache
        mountPath: /var/cache/nginx
      - name: run
        mountPath: /var/run
      - name: tmp
        mountPath: /tmp
  volumes:
    - name: cache
      emptyDir:
        sizeLimit: 128Mi
    - name: run
      emptyDir:
        sizeLimit: 8Mi
    - name: tmp
      emptyDir:
        sizeLimit: 64Mi
EOF
# R√©sultat attendu : pod/test-compliant created

kubectl get pod test-compliant
# STATUS doit √™tre Running

# Nettoyer
kubectl delete pod test-compliant
```

### Test 4 : V√©rifier que les namespaces syst√®me ne sont pas impact√©s

```bash
kubectl get pods -n kube-system
kubectl get pods -n longhorn-system
kubectl get pods -n traefik
# Tous les pods doivent √™tre Running
```

### Test 5 : Consulter les violations Audit remont√©es sur les namespaces infra

```bash
# Lister les PolicyReports par namespace infra
kubectl get policyreport -n longhorn-system
kubectl get policyreport -n traefik
kubectl get policyreport -n monitoring

# Voir le d√©tail des violations (r√©sultat "fail" = violation Audit)
kubectl get policyreport -A -o json | \
  jq '.items[] | {namespace: .metadata.namespace, fails: [.results[] | select(.result == "fail") | {policy: .policy, rule: .rule, resource: .resources[0].name, message: .message}]}'

# Surveiller les events Kyverno en temps r√©el
kubectl get events -A --field-selector reason=PolicyViolation
```

> Les violations remont√©es sur les namespaces infra sont normales et attendues : elles indiquent les composants qui ne respectent pas encore les standards (ex: node-exporter utilise `hostNetwork`, Longhorn utilise des containers root). Aucun pod n'est bloqu√©.
> 

---

## Policies Kyverno d√©ploy√©es

| Policy | Namespaces syst√®me | Namespaces infra | Namespaces applicatifs |
| --- | --- | --- | --- |
| `require-resources` | Exclus (webhook + background) | `Audit` | `Enforce` |
| `require-security-context` | Exclus (webhook + background) | `Audit` | `Enforce` |
| `disallow-privileged-containers` | Exclus (webhook + background) | `Audit` | `Enforce` |
| `disallow-host-namespaces` | Exclus (webhook + background) | `Audit` | `Enforce` |
| `require-storage-size` | Exclus (webhook + background) | `Audit` | `Enforce` |

---

## Phase 8 : Isolation r√©seau avec NetworkPolicies

### √âtape 23 ‚Äî Strat√©gie d'isolation r√©seau

Par d√©faut, Kubernetes autorise toute communication entre tous les pods de tous les namespaces. Ce comportement n'est pas viable en production. La strat√©gie mise en place ici est la suivante :

**Principe : deny-all par d√©faut, allow au cas par cas.**

Chaque namespace infra et applicatif est √©tanche. Les flux autoris√©s sont d√©finis explicitement. Les namespaces syst√®me Kubernetes (`kube-system`, `kube-public`, `kube-node-lease`) ne re√ßoivent **aucune** NetworkPolicy : les composants du control plane ont des besoins de communication complexes et non document√©s exhaustivement ; toute restriction est susceptible de casser le cluster.

**Pr√©requis :** Flannel (r√©seau overlay de K3s) supporte les NetworkPolicies via le CNI par d√©faut. Aucune installation suppl√©mentaire n'est n√©cessaire.

> **Note sur les NetworkPolicies et hostPort :** Traefik utilise `hostPort` ‚Äî le trafic HTTP/HTTPS arrive directement depuis le kernel de l'h√¥te, pas depuis un pod. Les NetworkPolicies s'appliquent au trafic pod-√†-pod. Pour Traefik, la r√®gle Ingress n'a donc pas de `from` : elle autorise le trafic depuis n'importe quelle source, la restriction √©tant assur√©e par UFW au niveau du n≈ìud.
> 

> **Note sur Prometheus et node-exporter :** `node-exporter` tourne en `hostNetwork` et expose ses m√©triques sur l'IP du n≈ìud. Prometheus le scrape via l'IP du n≈ìud, pas via un pod IP. La r√®gle Egress de Prometheus n'a donc pas de `to` pour ce cas : elle autorise la sortie sur le port 9100 vers n'importe quelle destination.
> 

**Depuis votre poste de travail.**

### Cr√©er les futurs namespaces

```bash
kubectl create ns loki
kubectl create ns monitoring
```

### Cr√©er la structure

```bash
mkdir -p infrastructure/network-policies
```

---

### `default-deny.yaml` ‚Äî Deny-all sur le namespace default

```bash
cat > infrastructure/network-policies/default-deny.yaml <<'EOF'
---
# Deny-all sur le namespace default.
# Pour chaque nouveau namespace applicatif : dupliquer ce fichier,
# changer le namespace, et ajouter les r√®gles allow n√©cessaires.
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-all
  namespace: default
spec:
  podSelector: {}
  policyTypes:
    - Ingress
    - Egress
---
# DNS : tous les pods du namespace default peuvent r√©soudre des noms.
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-dns-egress
  namespace: default
spec:
  podSelector: {}
  policyTypes:
    - Egress
  egress:
    - ports:
        - protocol: UDP
          port: 53
        - protocol: TCP
          port: 53
EOF
```

> `kube-system` ne re√ßoit aucune NetworkPolicy. Les NetworkPolicies appliqu√©es dans `kube-system` risquent de bloquer des composants critiques du control plane K3s dont les flux ne sont pas tous document√©s.
> 

---

### `traefik.yaml` ‚Äî NetworkPolicies pour Traefik

```bash
cat > infrastructure/network-policies/traefik.yaml <<'EOF'
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-all
  namespace: traefik
spec:
  podSelector: {}
  policyTypes:
    - Ingress
    - Egress
---
# Traefik re√ßoit le trafic HTTP/HTTPS via hostPort depuis l'h√¥te.
# Le trafic hostPort arrive au niveau du kernel, sans source namespace.
# La r√®gle Ingress n'a donc pas de "from" : UFW sur le n≈ìud assure
# la restriction au niveau IP (ports 80/443 ouverts, reste bloqu√©).
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-traefik-ingress
  namespace: traefik
spec:
  podSelector:
    matchLabels:
      app.kubernetes.io/name: traefik
  policyTypes:
    - Ingress
  ingress:
    - ports:
        - protocol: TCP
          port: 8000  # entrypoint web (hostPort 80)
        - protocol: TCP
          port: 8443  # entrypoint websecure (hostPort 443)
    # Prometheus scrape les m√©triques Traefik
    - from:
        - namespaceSelector:
            matchLabels:
              kubernetes.io/metadata.name: monitoring
      ports:
        - protocol: TCP
          port: 9100  # metrics Traefik
---
# Traefik doit joindre les backends applicatifs dans tous les namespaces.
# On autorise l'egress vers tous les ports applicatifs courants + DNS + API.
# L'isolation entre namespaces applicatifs est assur√©e par leurs propres
# NetworkPolicies (qui n'autorisent l'ingress que depuis le namespace traefik).
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-traefik-egress
  namespace: traefik
spec:
  podSelector:
    matchLabels:
      app.kubernetes.io/name: traefik
  policyTypes:
    - Egress
  egress:
    # DNS
    - ports:
        - protocol: UDP
          port: 53
        - protocol: TCP
          port: 53
    # Backends applicatifs (ports courants des apps web et APIs)
    - ports:
        - protocol: TCP
          port: 80
        - protocol: TCP
          port: 443
          # cert-manager HTTP-01 solver
        - protocol: TCP
          port: 8089
        - protocol: TCP
          port: 3000
        - protocol: TCP
          port: 8080
        - protocol: TCP
          port: 8443
        - protocol: TCP
          port: 9000
    # API Kubernetes (pour la d√©couverte des Ingress/IngressRoute)
    - ports:
        - protocol: TCP
          port: 6443
EOF
```

---

### `cert-manager.yaml` ‚Äî NetworkPolicies pour cert-manager

```bash
cat > infrastructure/network-policies/cert-manager.yaml <<'EOF'
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-all
  namespace: cert-manager
spec:
  podSelector: {}
  policyTypes:
    - Ingress
    - Egress
---
# cert-manager controller : doit joindre Let's Encrypt (ACME) et l'API Kubernetes.
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-cert-manager-controller-egress
  namespace: cert-manager
spec:
  podSelector:
    matchLabels:
      app.kubernetes.io/component: controller
  policyTypes:
    - Egress
  egress:
    - ports:
        - protocol: UDP
          port: 53
        - protocol: TCP
          port: 53
    # Let's Encrypt ACME + validations HTTP-01 via Traefik
    - ports:
        - protocol: TCP
          port: 443
        - protocol: TCP
          port: 80
    # API Kubernetes
    - ports:
        - protocol: TCP
          port: 6443
---
# cert-manager webhook : appel√© par le kube-apiserver pour valider les CRD.
# Le webhook √©coute sur le port 10250 (configur√© dans la chart cert-manager).
# L'apiserver K3s initie la connexion depuis l'IP du n≈ìud ‚Üí le webhook doit
# accepter du trafic entrant sans restriction de source namespace.
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-cert-manager-webhook
  namespace: cert-manager
spec:
  podSelector:
    matchLabels:
      app.kubernetes.io/component: webhook
  policyTypes:
    - Ingress
    - Egress
  ingress:
    - ports:
        - protocol: TCP
          port: 10250  # webhook cert-manager
  egress:
    - ports:
        - protocol: UDP
          port: 53
        - protocol: TCP
          port: 53
    - ports:
        - protocol: TCP
          port: 6443
---
# cert-manager cainjector : lit et injecte les CA dans les webhooks.
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-cert-manager-cainjector-egress
  namespace: cert-manager
spec:
  podSelector:
    matchLabels:
      app.kubernetes.io/component: cainjector
  policyTypes:
    - Egress
  egress:
    - ports:
        - protocol: UDP
          port: 53
        - protocol: TCP
          port: 53
    - ports:
        - protocol: TCP
          port: 6443
---
# Prometheus scrape les m√©triques cert-manager (port 9402 par d√©faut).
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-metrics-scraping
  namespace: cert-manager
spec:
  podSelector: {}
  policyTypes:
    - Ingress
  ingress:
    - from:
        - namespaceSelector:
            matchLabels:
              kubernetes.io/metadata.name: monitoring
      ports:
        - protocol: TCP
          port: 9402
EOF
```

---

### `argocd.yaml` ‚Äî NetworkPolicies pour ArgoCD

```bash
cat > infrastructure/network-policies/argocd.yaml <<'EOF'
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-all
  namespace: argocd
spec:
  podSelector: {}
  policyTypes:
    - Ingress
    - Egress
---
# ArgoCD server expose l'UI/API en HTTP (mode insecure, TLS termin√© par Traefik).
# Traefik forward le trafic vers argocd-server sur le port 8080.
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-argocd-server-ingress
  namespace: argocd
spec:
  podSelector:
    matchLabels:
      app.kubernetes.io/name: argocd-server
  policyTypes:
    - Ingress
  ingress:
    - from:
        - namespaceSelector:
            matchLabels:
              kubernetes.io/metadata.name: traefik
      ports:
        - protocol: TCP
          port: 8080
---
# ArgoCD egress : tous les composants (server, repo-server, application-controller,
# applicationset-controller, notifications-controller) ont besoin de DNS,
# de GitHub (443) et de l'API Kubernetes (6443).
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-argocd-egress
  namespace: argocd
spec:
  podSelector: {}
  policyTypes:
    - Egress
  egress:
    - ports:
        - protocol: UDP
          port: 53
        - protocol: TCP
          port: 53
    # GitHub, Helm registries, OCI registries
    - ports:
        - protocol: TCP
          port: 443
    # API Kubernetes (synchro des ressources)
    - ports:
        - protocol: TCP
          port: 6443
---
# Communication interne ArgoCD : les composants se parlent entre eux
# (server ‚Üî repo-server ‚Üî application-controller ‚Üî redis, etc.).
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-argocd-internal
  namespace: argocd
spec:
  podSelector: {}
  policyTypes:
    - Ingress
    - Egress
  ingress:
    - from:
        - podSelector: {}
  egress:
    - to:
        - podSelector: {}
---
# Prometheus scrape les m√©triques ArgoCD.
# argocd-server expose sur 8083, argocd-application-controller sur 8082.
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-metrics-scraping
  namespace: argocd
spec:
  podSelector: {}
  policyTypes:
    - Ingress
  ingress:
    - from:
        - namespaceSelector:
            matchLabels:
              kubernetes.io/metadata.name: monitoring
      ports:
        - protocol: TCP
          port: 8082  # application-controller metrics
        - protocol: TCP
          port: 8083  # server metrics
        - protocol: TCP
          port: 8084  # repo-server metrics
EOF
```

---

### `longhorn-system.yaml` ‚Äî NetworkPolicies pour Longhorn

```bash
cat > infrastructure/network-policies/longhorn-system.yaml <<'EOF'
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-all
  namespace: longhorn-system
spec:
  podSelector: {}
  policyTypes:
    - Ingress
    - Egress
---
# Communication interne Longhorn : manager, driver, engine, replica
# se parlent tous dans le m√™me namespace.
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-longhorn-internal
  namespace: longhorn-system
spec:
  podSelector: {}
  policyTypes:
    - Ingress
    - Egress
  ingress:
    - from:
        - podSelector: {}
  egress:
    - to:
        - podSelector: {}
---
# Longhorn egress vers l'ext√©rieur du namespace :
# - DNS pour la r√©solution des noms
# - API Kubernetes (6443) pour la gestion des PV/PVC/CSI
# - Port 10250 (kubelet) sur les n≈ìuds pour les op√©rations CSI
# - Ports iSCSI inter-n≈ìuds (Tailscale, donc IP du n≈ìud, pas pod IP)
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-longhorn-egress
  namespace: longhorn-system
spec:
  podSelector: {}
  policyTypes:
    - Egress
  egress:
    - ports:
        - protocol: UDP
          port: 53
        - protocol: TCP
          port: 53
    # API Kubernetes
    - ports:
        - protocol: TCP
          port: 6443
    # Kubelet sur les n≈ìuds (CSI node plugin)
    - ports:
        - protocol: TCP
          port: 10250
    # Ports Longhorn inter-n≈ìuds (engine/replica communication via Tailscale)
    - ports:
        - protocol: TCP
          port: 9500
        - protocol: TCP
          port: 9501
        - protocol: TCP
          port: 9502
        - protocol: TCP
          port: 9503
        - protocol: TCP
          port: 9504
---
# Longhorn ingress depuis l'ext√©rieur du namespace :
# - iSCSI et ports Longhorn : initiateurs iSCSI sur les n≈ìuds (hostNetwork)
# - Webhook : appel√© par le kube-apiserver depuis l'IP du n≈ìud
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-longhorn-ingress
  namespace: longhorn-system
spec:
  podSelector: {}
  policyTypes:
    - Ingress
  ingress:
    # Webhook admission (appel√© par kube-apiserver, source = IP n≈ìud)
    - ports:
        - protocol: TCP
          port: 9443
    # iSCSI et r√©plication Longhorn inter-n≈ìuds
    - ports:
        - protocol: TCP
          port: 3260
        - protocol: TCP
          port: 9500
        - protocol: TCP
          port: 9501
        - protocol: TCP
          port: 9502
        - protocol: TCP
          port: 9503
        - protocol: TCP
          port: 9504
---
# Prometheus scrape les m√©triques Longhorn (port 9500 du manager).
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-metrics-scraping
  namespace: longhorn-system
spec:
  podSelector:
    matchLabels:
      app: longhorn-manager
  policyTypes:
    - Ingress
  ingress:
    - from:
        - namespaceSelector:
            matchLabels:
              kubernetes.io/metadata.name: monitoring
      ports:
        - protocol: TCP
          port: 9500
EOF
```

---

### `kyverno.yaml` ‚Äî NetworkPolicies pour Kyverno

```bash
cat > infrastructure/network-policies/kyverno.yaml <<'EOF'
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-all
  namespace: kyverno
spec:
  podSelector: {}
  policyTypes:
    - Ingress
    - Egress
---
# Kyverno admission webhook : appel√© par le kube-apiserver depuis l'IP du n≈ìud.
# Pas de "from" car la source est l'apiserver (IP h√¥te, pas un pod).
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-kyverno-webhook-ingress
  namespace: kyverno
spec:
  podSelector:
    matchLabels:
      app.kubernetes.io/component: admission-controller
  policyTypes:
    - Ingress
  ingress:
    - ports:
        - protocol: TCP
          port: 9443
---
# Kyverno egress : DNS + API Kubernetes pour tous les composants
# (admission-controller, background-controller, cleanup-controller, reports-controller).
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-kyverno-egress
  namespace: kyverno
spec:
  podSelector: {}
  policyTypes:
    - Egress
  egress:
    - ports:
        - protocol: UDP
          port: 53
        - protocol: TCP
          port: 53
    - ports:
        - protocol: TCP
          port: 6443
---
# Communication interne Kyverno entre ses composants.
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-kyverno-internal
  namespace: kyverno
spec:
  podSelector: {}
  policyTypes:
    - Ingress
    - Egress
  ingress:
    - from:
        - podSelector: {}
  egress:
    - to:
        - podSelector: {}
---
# Prometheus scrape les m√©triques Kyverno (port 8000).
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-metrics-scraping
  namespace: kyverno
spec:
  podSelector: {}
  policyTypes:
    - Ingress
  ingress:
    - from:
        - namespaceSelector:
            matchLabels:
              kubernetes.io/metadata.name: monitoring
      ports:
        - protocol: TCP
          port: 8000
EOF
```

---

### `monitoring.yaml` ‚Äî NetworkPolicies pour Prometheus + Grafana

```bash
cat > infrastructure/network-policies/monitoring.yaml <<'EOF'
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-all
  namespace: monitoring
spec:
  podSelector: {}
  policyTypes:
    - Ingress
    - Egress
---
# Communication interne monitoring : Prometheus ‚Üî Alertmanager,
# Grafana ‚Üî Prometheus, Operator ‚Üî composants, etc.
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-monitoring-internal
  namespace: monitoring
spec:
  podSelector: {}
  policyTypes:
    - Ingress
    - Egress
  ingress:
    - from:
        - podSelector: {}
  egress:
    - to:
        - podSelector: {}
---
# Prometheus egress vers les cibles de scraping dans les autres namespaces.
# node-exporter tourne en hostNetwork : son IP est celle du n≈ìud, pas un pod IP.
# Les r√®gles sans "to" autorisent la sortie sur ce port vers toute destination
# (incluant les IPs des n≈ìuds Tailscale) ‚Äî UFW restreint ce qui arrive au n≈ìud.
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-prometheus-scraping-egress
  namespace: monitoring
spec:
  podSelector:
    matchLabels:
      app.kubernetes.io/name: prometheus
  policyTypes:
    - Egress
  egress:
    - ports:
        - protocol: UDP
          port: 53
        - protocol: TCP
          port: 53
    # API Kubernetes (ServiceMonitor discovery, kubelet metrics)
    - ports:
        - protocol: TCP
          port: 6443
        - protocol: TCP
          port: 10250  # kubelet /metrics/cadvisor
    # node-exporter (hostNetwork, IP du n≈ìud)
    - ports:
        - protocol: TCP
          port: 9100
    # M√©triques des namespaces infra
    - ports:
        - protocol: TCP
          port: 8000   # kyverno
        - protocol: TCP
          port: 8080   # argocd applicationset, divers
        - protocol: TCP
          port: 8082   # argocd application-controller
        - protocol: TCP
          port: 8083   # argocd server
        - protocol: TCP
          port: 8084   # argocd repo-server
        - protocol: TCP
          port: 9100   # traefik metrics
        - protocol: TCP
          port: 9402   # cert-manager
        - protocol: TCP
          port: 9500   # longhorn manager
        - protocol: TCP
          port: 3100   # loki
---
# cert-manager HTTP-01 solver : re√ßoit le trafic depuis Traefik
# √Ä inclure dans tout namespace ayant des certificats Let's Encrypt
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-acme-solver-ingress
  namespace: monitoring  
spec:
  podSelector:
    matchLabels:
      acme.cert-manager.io/http01-solver: "true"
  policyTypes:
    - Ingress
  ingress:
    - from:
        - namespaceSelector:
            matchLabels:
              kubernetes.io/metadata.name: traefik
      ports:
        - protocol: TCP
          port: 8089
---
# Acc√®s grafana.com pour cl√©s de signature
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-grafana-internet-egress
  namespace: monitoring
spec:
  podSelector:
    matchLabels:
      app.kubernetes.io/name: grafana
      app.kubernetes.io/instance: kube-prometheus-stack
  policyTypes:
    - Egress
  egress:
    - ports:
        - protocol: UDP
          port: 53
        - protocol: TCP
          port: 53
    - to:
        - podSelector:
            matchLabels:
              app.kubernetes.io/name: prometheus
      ports:
        - protocol: TCP
          port: 9090
    - to:
        - namespaceSelector:
            matchLabels:
              kubernetes.io/metadata.name: loki
      ports:
        - protocol: TCP
          port: 3100
    - ports:
        - protocol: TCP
          port: 443
---
# Acc√®s API server pour tous les composants monitoring
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-apiserver-egress
  namespace: monitoring
spec:
  podSelector: {}
  policyTypes:
    - Egress
  egress:
    - ports:
        - protocol: TCP
          port: 443
        - protocol: TCP
          port: 6443
---
# Grafana : re√ßoit le trafic depuis Traefik (UI).
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-grafana-ingress
  namespace: monitoring
spec:
  podSelector:
    matchLabels:
      app.kubernetes.io/name: grafana
  policyTypes:
    - Ingress
  ingress:
    - from:
        - namespaceSelector:
            matchLabels:
              kubernetes.io/metadata.name: traefik
      ports:
        - protocol: TCP
          port: 3000
---
# Grafana egress : DNS + Loki (datasource externe au namespace).
# Prometheus est dans le m√™me namespace ‚Üí couvert par allow-monitoring-internal.
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-grafana-egress
  namespace: monitoring
spec:
  podSelector:
    matchLabels:
      app.kubernetes.io/name: grafana
  policyTypes:
    - Egress
  egress:
    - ports:
        - protocol: UDP
          port: 53
        - protocol: TCP
          port: 53
    # Loki (namespace loki)
    - to:
        - namespaceSelector:
            matchLabels:
              kubernetes.io/metadata.name: loki
      ports:
        - protocol: TCP
          port: 3100
EOF
```

---

### `loki.yaml` ‚Äî NetworkPolicies pour Loki + Promtail

```bash
cat > infrastructure/network-policies/loki.yaml <<'EOF'
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-all
  namespace: loki
spec:
  podSelector: {}
  policyTypes:
    - Ingress
    - Egress
---
# Loki ingress : re√ßoit les logs de Promtail (m√™me namespace)
# et les requ√™tes de Grafana (namespace monitoring).
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-loki-ingress
  namespace: loki
spec:
  podSelector:
    matchLabels:
      app.kubernetes.io/name: loki
  policyTypes:
    - Ingress
  ingress:
    # Promtail est dans le m√™me namespace
    - from:
        - podSelector:
            matchLabels:
              app.kubernetes.io/name: promtail
      ports:
        - protocol: TCP
          port: 3100
    # Grafana est dans le namespace monitoring
    - from:
        - namespaceSelector:
            matchLabels:
              kubernetes.io/metadata.name: monitoring
      ports:
        - protocol: TCP
          port: 3100
---
# Loki egress : DNS et API Server
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-loki-egress
  namespace: loki
spec:
  podSelector:
    matchLabels:
      app.kubernetes.io/name: loki
  policyTypes:
    - Egress
  egress:
    - ports:
        - protocol: UDP
          port: 53
        - protocol: TCP
          port: 53
        # Permet les flux depuis les pods Loki vers l'API Server
        - protocol: TCP
          port: 6443
---
# Promtail egress : DNS + API Kubernetes (d√©couverte des pods pour les labels)
# + Loki (envoi des logs).
# Promtail lit les logs depuis un hostPath (/var/log/pods) ‚Äî cela ne g√©n√®re
# pas de trafic r√©seau, pas besoin de r√®gle pour √ßa.
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-promtail-egress
  namespace: loki
spec:
  podSelector:
    matchLabels:
      app.kubernetes.io/name: promtail
  policyTypes:
    - Egress
  egress:
    - ports:
        - protocol: UDP
          port: 53
        - protocol: TCP
          port: 53
    # API Kubernetes : d√©couverte des pods et labels
    - ports:
        - protocol: TCP
          port: 6443
    # Loki : envoi des logs (m√™me namespace, on cible le pod directement)
    - to:
        - podSelector:
            matchLabels:
              app.kubernetes.io/name: loki
      ports:
        - protocol: TCP
          port: 3100
---
# Prometheus scrape les m√©triques Loki et Promtail.
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-metrics-scraping
  namespace: loki
spec:
  podSelector: {}
  policyTypes:
    - Ingress
  ingress:
    - from:
        - namespaceSelector:
            matchLabels:
              kubernetes.io/metadata.name: monitoring
      ports:
        - protocol: TCP
          port: 3100  # loki metrics
        - protocol: TCP
          port: 3101  # promtail metrics
EOF
```

---

### `app-namespace-template.yaml` ‚Äî Template pour les namespaces applicatifs

```bash
cat > infrastructure/network-policies/app-namespace-template.yaml <<'EOF'
# =============================================================
# Template NetworkPolicy pour un namespace applicatif.
# Dupliquer et adapter pour chaque namespace d'application.
# Ne pas inclure ce fichier dans le kustomization.yaml.
# =============================================================
---
# 1. Deny-all ingress et egress
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-all
  namespace: mon-namespace  # ‚ö†Ô∏è REMPLACER
spec:
  podSelector: {}
  policyTypes:
    - Ingress
    - Egress
---
# 2. DNS : r√©solution de noms pour tous les pods du namespace
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-dns-egress
  namespace: mon-namespace  # ‚ö†Ô∏è REMPLACER
spec:
  podSelector: {}
  policyTypes:
    - Egress
  egress:
    - ports:
        - protocol: UDP
          port: 53
        - protocol: TCP
          port: 53
---
# 3. Ingress depuis Traefik (trafic HTTP applicatif)
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-ingress-from-traefik
  namespace: mon-namespace  # ‚ö†Ô∏è REMPLACER
spec:
  podSelector:
    matchLabels:
      app: mon-app  # ‚ö†Ô∏è REMPLACER par le label de votre pod applicatif
  policyTypes:
    - Ingress
  ingress:
    - from:
        - namespaceSelector:
            matchLabels:
              kubernetes.io/metadata.name: traefik
      ports:
        - protocol: TCP
          port: 8080  # ‚ö†Ô∏è REMPLACER par le port d'√©coute de votre app
---
# 4. Scraping Prometheus
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-metrics-scraping
  namespace: mon-namespace  # ‚ö†Ô∏è REMPLACER
spec:
  podSelector: {}
  policyTypes:
    - Ingress
  ingress:
    - from:
        - namespaceSelector:
            matchLabels:
              kubernetes.io/metadata.name: monitoring
      ports:
        - protocol: TCP
          port: 9090  # ‚ö†Ô∏è REMPLACER par le port m√©triques de votre app
EOF
```

---

### `kustomization.yaml` ‚Äî Kustomization NetworkPolicies

```bash
cat > infrastructure/network-policies/kustomization.yaml <<'EOF'
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - default-deny.yaml
  - traefik.yaml
  - cert-manager.yaml
  - argocd.yaml
  - longhorn-system.yaml
  - kyverno.yaml
  - monitoring.yaml
  - loki.yaml
  # Ne pas inclure app-namespace-template.yaml (template de r√©f√©rence uniquement)
  # kube-system n'a intentionnellement pas de NetworkPolicies (voir note ci-dessus)
EOF
```

### Cr√©er l'Application ArgoCD

```bash
cat > argocd-apps/network-policies.yaml <<EOF
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: network-policies
  namespace: argocd
  finalizers:
    - resources-finalizer.argocd.argoproj.io
spec:
  project: default

  sources:
    - repoURL: '$GIT_URL'
      targetRevision: main
      path: infrastructure/network-policies

  destination:
    server: 'https://kubernetes.default.svc'

  syncPolicy:
    automated:
      prune: true
      selfHeal: true
      allowEmpty: false
    syncOptions:
      - ServerSideApply=true
    retry:
      limit: 5
      backoff:
        duration: 5s
        factor: 2
        maxDuration: 3m
EOF
```

> **Pas de `destination.namespace`** car les NetworkPolicies ciblent des namespaces diff√©rents. Chaque manifeste sp√©cifie son propre namespace.
> 

---

## √âtape 24 ‚Äî D√©ployer les NetworkPolicies via ArgoCD

**Depuis votre poste de travail.**

```bash
cd ~/k3s-gitops
git add infrastructure/network-policies/ argocd-apps/network-policies.yaml
git commit -m "Add NetworkPolicies: deny-all by default, per-namespace allow rules"
git push origin main

kubectl apply -f argocd-apps/network-policies.yaml

kubectl get application -n argocd network-policies -w
```

### V√©rification

```bash
# Lister toutes les NetworkPolicies d√©ploy√©es
kubectl get networkpolicies -A

# V√©rifier que tous les pods sont toujours Running
kubectl get pods -A
# Aucun pod ne doit passer en CrashLoopBackOff ou Pending

# V√©rifier ArgoCD (acc√®s UI)
curl -s -o /dev/null -w "%{http_code}" https://argocd.votredomaine.com
# R√©sultat attendu : 200

# V√©rifier que Traefik r√©pond (HTTP ‚Üí HTTPS redirect)
curl -I http://<IP_PUBLIQUE_NOEUD1>
# R√©sultat attendu : 308 Permanent Redirect

# Tester l'isolation : un pod dans default ne peut pas joindre argocd
kubectl run test-isolation --image=busybox:1.36 --rm -it --restart=Never -n default \
  -- wget -qO- --timeout=3 http://argocd-server.argocd.svc:80
# R√©sultat attendu : timeout (bloqu√© par les NetworkPolicies)
```

---

## NetworkPolicies d√©ploy√©es

| Namespace | R√®gles | Acc√®s autoris√©s |
| --- | --- | --- |
| `kube-system` | Aucune (intentionnel) | Non restreint ‚Äî composants K3s critiques |
| `default` | `default-deny-all`, `allow-dns-egress` | DNS sortant uniquement |
| `traefik` | `default-deny-all`, `allow-traefik-ingress`, `allow-traefik-egress` | hostPort 80/443 entrant, backends applicatifs et API sortant |
| `cert-manager` | `default-deny-all`, `allow-cert-manager-controller-egress`, `allow-cert-manager-webhook`, `allow-cert-manager-cainjector-egress`, `allow-metrics-scraping` | ACME Let's Encrypt, API Kubernetes, webhook |
| `argocd` | `default-deny-all`, `allow-argocd-server-ingress`, `allow-argocd-egress`, `allow-argocd-internal`, `allow-metrics-scraping` | UI via Traefik, GitHub, API Kubernetes, communication interne |
| `longhorn-system` | `default-deny-all`, `allow-longhorn-internal`, `allow-longhorn-egress`, `allow-longhorn-ingress`, `allow-metrics-scraping` | Communication inter-pods, API Kubernetes, iSCSI inter-n≈ìuds |
| `kyverno` | `default-deny-all`, `allow-kyverno-webhook-ingress`, `allow-kyverno-egress`, `allow-kyverno-internal`, `allow-metrics-scraping` | Webhook admission, API Kubernetes, communication interne |
| `monitoring` | `default-deny-all`, `allow-monitoring-internal`, `allow-prometheus-scraping-egress`, `allow-grafana-ingress`, `allow-grafana-egress` | Scraping tous namespaces, UI Grafana via Traefik, Loki datasource |
| `loki` | `default-deny-all`, `allow-loki-ingress`, `allow-loki-egress`, `allow-promtail-egress`, `allow-metrics-scraping` | Ingestion Promtail, requ√™tes Grafana, API Kubernetes pour discovery |

---

## Phase 9 : Monitoring avec Prometheus et Grafana

### √âtape 25 ‚Äî Pr√©parer le d√©ploiement de kube-prometheus-stack

La chart communautaire `kube-prometheus-stack` regroupe Prometheus, Grafana, Alertmanager, les exporters (node-exporter, kube-state-metrics) et un ensemble de dashboards et r√®gles d‚Äôalerting pr√©configur√©s pour le monitoring Kubernetes. Le tout est orchestr√© par le Prometheus Operator via des CRD d√©di√©es (ServiceMonitor, PodMonitor, PrometheusRule).

**Ce qui sera d√©ploy√© :**

- **Prometheus** ‚Äî collecte des m√©triques du cluster, des n≈ìuds et des workloads
- **Grafana** ‚Äî interface de visualisation avec dashboards Kubernetes, Traefik, Longhorn et Loki pr√©configur√©s
- **Alertmanager** ‚Äî gestion et routage des alertes
- **node-exporter** ‚Äî m√©triques syst√®me de chaque n≈ìud (CPU, m√©moire, disque, r√©seau)
- **kube-state-metrics** ‚Äî m√©triques sur l‚Äô√©tat des objets Kubernetes (pods, deployments, etc.)
- **Prometheus Operator** ‚Äî g√®re le cycle de vie des composants via des CRD

**Pr√©requis :** Longhorn op√©rationnel (Phase 4) pour la persistence des donn√©es Prometheus et Grafana. cert-manager op√©rationnel (Phase 6) pour l‚Äôexposition Grafana en HTTPS.

**Depuis votre poste de travail.**

### Cr√©er le values.yaml

```bash
mkdir -p infrastructure/monitoring

cat > infrastructure/monitoring/values.yaml <<'EOF'
# Configuration kube-prometheus-stack pour K3s 3 masters

# --- CRDs ---
crds:
  enabled: true

# --- Prometheus ---
prometheus:
  prometheusSpec:
    replicas: 1
    retention: 15d
    retentionSize: "8GB"
    additionalScrapeConfigs:
      - job_name: 'etcd'
        static_configs:
          - targets:
              - '100.112.1.80:2382'  # ‚ö†Ô∏è IP Tailscale n≈ìud 1
              - '100.112.179.112:2382'  # ‚ö†Ô∏è IP Tailscale n≈ìud 2
              - '100.72.21.63:2382'  # ‚ö†Ô∏è IP Tailscale n≈ìud 3
    storageSpec:
      volumeClaimTemplate:
        spec:
          storageClassName: longhorn
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 10Gi

    resources:
      requests:
        cpu: 200m
        memory: 1Gi
      limits:
        cpu: 1000m
        memory: 2Gi

    tolerations:
      - key: node-role.kubernetes.io/control-plane
        operator: Exists
        effect: NoSchedule
      - key: node-role.kubernetes.io/master
        operator: Exists
        effect: NoSchedule

    # S√©lectionner les ServiceMonitors/PodMonitors de tous les namespaces
    serviceMonitorSelectorNilUsesHelmValues: false
    podMonitorSelectorNilUsesHelmValues: false
    ruleSelectorNilUsesHelmValues: false

# --- Alertmanager ---
alertmanager:
  alertmanagerSpec:
    replicas: 1
    retention: 120h

    storage:
      volumeClaimTemplate:
        spec:
          storageClassName: longhorn
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 2Gi

    resources:
      requests:
        cpu: 50m
        memory: 64Mi
      limits:
        cpu: 200m
        memory: 256Mi

    tolerations:
      - key: node-role.kubernetes.io/control-plane
        operator: Exists
        effect: NoSchedule
      - key: node-role.kubernetes.io/master
        operator: Exists
        effect: NoSchedule

# --- Grafana ---
grafana:
  replicas: 1

  adminPassword: ""  # ‚ö†Ô∏è REMPLACER par un mot de passe fort, ou laisser vide pour un g√©n√©r√© al√©atoirement

  persistence:
    enabled: true
    storageClassName: longhorn
    size: 2Gi

  resources:
    requests:
      cpu: 100m
      memory: 128Mi
    limits:
      cpu: 500m
      memory: 512Mi

  tolerations:
    - key: node-role.kubernetes.io/control-plane
      operator: Exists
      effect: NoSchedule
    - key: node-role.kubernetes.io/master
      operator: Exists
      effect: NoSchedule

  # Dashboards par d√©faut (inclus dans la chart)
  defaultDashboardsEnabled: true
  defaultDashboardsTimezone: Europe/Paris

  # Datasources
  sidecar:
    dashboards:
      enabled: true
      searchNamespace: ALL
    datasources:
      enabled: true

  # Datasource Loki (ajout√©e manuellement car Loki n'est pas dans la chart)
  additionalDataSources:
    - name: Loki
      type: loki
      url: http://loki.loki.svc:3100
      access: proxy
      isDefault: false
      jsonData:
        maxLines: 1000

  # Dashboards suppl√©mentaires via ConfigMaps (Traefik, Longhorn, Loki)
  dashboardProviders:
    dashboardproviders.yaml:
      apiVersion: 1
      providers:
        - name: 'default'
          orgId: 1
          folder: ''
          type: file
          disableDeletion: false
          editable: true
          options:
            path: /var/lib/grafana/dashboards/default
        - name: 'custom'
          orgId: 1
          folder: 'Custom'
          type: file
          disableDeletion: false
          editable: true
          options:
            path: /var/lib/grafana/dashboards/custom

  dashboards:
    custom:
      traefik-official:
        gnetId: 17346
        revision: 9
        datasource: Prometheus
      longhorn:
        gnetId: 13032
        revision: 6
        datasource: Prometheus
      loki-logs:
        gnetId: 13639
        revision: 2
        datasource: Loki
      loki-container-log:
        gnetId: 16966
        revision: 1
        datasource: Loki

  # Ingress Grafana (expos√© via Traefik + cert-manager)
  ingress:
    enabled: true
    annotations:
      cert-manager.io/cluster-issuer: "letsencrypt-prod"
      traefik.ingress.kubernetes.io/router.entrypoints: websecure
      traefik.ingress.kubernetes.io/router.tls: "true"
      traefik.ingress.kubernetes.io/router.middlewares: traefik-default-security-chain@kubernetescrd
    hosts:
      - grafana.votredomaine.com  # ‚ö†Ô∏è REMPLACER
    tls:
      - secretName: grafana-tls
        hosts:
          - grafana.votredomaine.com  # ‚ö†Ô∏è REMPLACER

# --- Prometheus Operator ---
prometheusOperator:
  resources:
    requests:
      cpu: 100m
      memory: 128Mi
    limits:
      cpu: 300m
      memory: 256Mi

  tolerations:
    - key: node-role.kubernetes.io/control-plane
      operator: Exists
      effect: NoSchedule
    - key: node-role.kubernetes.io/master
      operator: Exists
      effect: NoSchedule

# --- node-exporter (DaemonSet sur tous les n≈ìuds) ---
prometheus-node-exporter:
  resources:
    requests:
      cpu: 50m
      memory: 32Mi
    limits:
      cpu: 200m
      memory: 64Mi

  tolerations:
    - key: node-role.kubernetes.io/control-plane
      operator: Exists
      effect: NoSchedule
    - key: node-role.kubernetes.io/master
      operator: Exists
      effect: NoSchedule

# --- kube-state-metrics ---
kube-state-metrics:
  resources:
    requests:
      cpu: 50m
      memory: 64Mi
    limits:
      cpu: 200m
      memory: 128Mi

  tolerations:
    - key: node-role.kubernetes.io/control-plane
      operator: Exists
      effect: NoSchedule
    - key: node-role.kubernetes.io/master
      operator: Exists
      effect: NoSchedule

# --- D√©sactiver les composants inutiles pour notre setup ---
kubeProxy:
  enabled: false  # K3s n'utilise pas kube-proxy (remplac√© par kube-router/iptables)

kubeEtcd:
  enabled: false  # etcd K3s n'expose pas les m√©triques par d√©faut

kubeControllerManager:
  enabled: false  # Non accessible dans K3s

kubeScheduler:
  enabled: false  # Non accessible dans K3s
EOF
```

> **Note sur les composants d√©sactiv√©s :** K3s int√®gre le control plane dans un seul binaire. Les m√©triques de kube-controller-manager, kube-scheduler et kube-proxy ne sont pas expos√©es de la m√™me mani√®re que dans un cluster kubeadm standard. Les d√©sactiver √©vite des alertes `TargetDown` permanentes.
> 

> **Note sur les dashboards :** Quatre dashboards sont provisionn√©s automatiquement via grafana.com : le dashboard officiel Traefik (ID 17346), le dashboard Longhorn (ID 13032), et deux dashboards Loki pour l‚Äôexploration des logs (ID 13639, 16966). Ils appara√Ætront dans le dossier ‚ÄúCustom‚Äù de Grafana.
> 

```bash
# Remplacer le domaine Grafana
sed -i 's|grafana.votredomaine.com|grafana.votreVRAIdomaine.com|g' infrastructure/monitoring/values.yaml
```

### Cr√©er l‚ÄôApplication ArgoCD

```bash
cat > argocd-apps/monitoring.yaml <<EOF
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: monitoring
  namespace: argocd
  finalizers:
    - resources-finalizer.argocd.argoproj.io
spec:
  project: default

  sources:
    - repoURL: 'https://prometheus-community.github.io/helm-charts'
      targetRevision: 82.0.0
      chart: kube-prometheus-stack
      helm:
        releaseName: kube-prometheus-stack
        valueFiles:
          - '\$values/infrastructure/monitoring/values.yaml'

    - repoURL: '$GIT_URL'
      targetRevision: main
      ref: values

  destination:
    server: 'https://kubernetes.default.svc'
    namespace: monitoring

  syncPolicy:
    automated:
      prune: true
      selfHeal: true
      allowEmpty: false
    syncOptions:
      - CreateNamespace=true
      - ServerSideApply=true
    retry:
      limit: 5
      backoff:
        duration: 5s
        factor: 2
        maxDuration: 3m
EOF
```

---

### √âtape 26 ‚Äî D√©ployer kube-prometheus-stack via ArgoCD

**Depuis votre poste de travail.**

### V√©rifier le DNS pour Grafana

```bash
dig +short grafana.votredomaine.com
# R√©sultat attendu : les 3 IP publiques
```

### Pousser et d√©ployer

```bash
cd ~/k3s-gitops
git add infrastructure/monitoring/ argocd-apps/monitoring.yaml
git commit -m "Add kube-prometheus-stack with Traefik, Longhorn and Loki dashboards"
git push origin main

kubectl apply -f argocd-apps/monitoring.yaml

# Suivre la synchronisation (3-5 minutes, la chart est volumineuse)
kubectl get application -n argocd monitoring -w
```

> Les CRDs du Prometheus Operator peuvent prendre un moment √† s‚Äôinstaller. ArgoCD retente automatiquement gr√¢ce √† la politique de retry.
> 

### V√©rification

```bash
# Tous les pods doivent √™tre Running
kubectl get pods -n monitoring

# StatefulSets (Prometheus et Alertmanager)
kubectl get statefulsets -n monitoring

# DaemonSet node-exporter (3 pods, un par n≈ìud)
kubectl get daemonset -n monitoring

# PVC (Prometheus, Alertmanager, Grafana doivent √™tre Bound)
kubectl get pvc -n monitoring

# CRDs Prometheus Operator
kubectl get crds | grep monitoring.coreos.com
# servicemonitors, podmonitors, prometheusrules, alertmanagerconfigs, etc.

# V√©rifier que les ServiceMonitors des composants existants sont d√©tect√©s
kubectl get servicemonitors -A
# R√©sultat attendu : servicemonitors dans traefik, longhorn-system, cert-manager, kyverno
```

### Activer le monitoring sur les solutions pr√©c√©dement d√©ploy√©es

TODO

### R√©cup√©rer le mot de passe Grafana

Si vous avez laiss√© `adminPassword` vide, la chart en g√©n√®re un al√©atoirement :

```bash
kubectl get secret -n monitoring kube-prometheus-stack-grafana -o jsonpath="{.data.admin-password}" | base64 -d && echo
```

### Connexion √† Grafana

Ouvrir `https://grafana.votredomaine.com` dans un navigateur. Se connecter avec `admin` et le mot de passe r√©cup√©r√©.

### Dashboards disponibles

Les dashboards pr√©configur√©s sont accessibles via le menu Dashboards :

**Dashboards Kubernetes (dossier par d√©faut) :**

- **Kubernetes / Compute Resources / Cluster** ‚Äî vue globale CPU/m√©moire du cluster
- **Kubernetes / Compute Resources / Namespace (Pods)** ‚Äî ressources par namespace et par pod
- **Kubernetes / Compute Resources / Node (Pods)** ‚Äî charge par n≈ìud
- **Node Exporter / Nodes** ‚Äî m√©triques syst√®me d√©taill√©es (CPU, m√©moire, disque, r√©seau)
- **Kubernetes / Networking / Cluster** ‚Äî trafic r√©seau
- **Kubernetes / Persistent Volumes** ‚Äî √©tat et usage des volumes Longhorn

**Dashboards Custom (dossier Custom) :**

- **Traefik Official** ‚Äî requ√™tes par entrypoint, status codes, latence par service, certificats TLS
- **Longhorn** ‚Äî sant√© des volumes, r√©plication, IOPS, espace disque par n≈ìud
- **Loki Logs** ‚Äî exploration des logs par namespace/pod/container (disponible apr√®s la Phase 10)
- **Loki Container Log** ‚Äî vue d√©taill√©e par container avec filtrage temps r√©el

> **Note :** Le dashboard Loki affichera ‚ÄúNo data‚Äù tant que Loki n‚Äôest pas d√©ploy√© (Phase 10). C‚Äôest normal.
> 

---

## Phase 10 : Agr√©gation des logs avec Loki et Promtail

### √âtape 27 ‚Äî Pr√©parer le d√©ploiement de Loki

Loki est un syst√®me d‚Äôagr√©gation de logs con√ßu par Grafana Labs. Contrairement √† Elasticsearch, Loki n‚Äôindexe pas le contenu des logs mais uniquement les labels (namespace, pod, container), ce qui le rend l√©ger en ressources et adapt√© √† un cluster de 3 n≈ìuds.

**Ce qui sera d√©ploy√© :**

- **Loki** ‚Äî serveur de stockage et d‚Äôinterrogation des logs (mode SingleBinary)
- **Promtail** ‚Äî agent DaemonSet qui collecte les logs de tous les containers sur chaque n≈ìud et les pousse vers Loki

**Architecture :** Loki est d√©ploy√© en mode SingleBinary (monolithique), adapt√© √† notre cluster de taille modeste. Les logs sont stock√©s sur le syst√®me de fichiers local via un PVC Longhorn. Promtail est d√©ploy√© en DaemonSet sur les 3 n≈ìuds.

**Pr√©requis :** Longhorn op√©rationnel (Phase 4) pour la persistence. Grafana op√©rationnel (Phase 9) avec la datasource Loki d√©j√† configur√©e.

**Depuis votre poste de travail.**

### Cr√©er le values.yaml Loki

```bash
mkdir -p infrastructure/loki

cat > infrastructure/loki/values.yaml <<'EOF'
# Configuration Loki pour K3s 3 masters ‚Äî mode SingleBinary

deploymentMode: SingleBinary

loki:
  auth_enabled: false

  commonConfig:
    replication_factor: 1

  storage:
    type: filesystem

  schemaConfig:
    configs:
      - from: "2024-01-01"
        store: tsdb
        object_store: filesystem
        schema: v13
        index:
          prefix: loki_index_
          period: 24h

  limits_config:
    retention_period: 168h  # 7 jours
    max_query_series: 500
    max_query_parallelism: 2
    ingestion_rate_mb: 4
    ingestion_burst_size_mb: 8

  compactor:
    working_directory: /var/loki/compactor
    compaction_interval: 10m
    retention_enabled: true
    retention_delete_delay: 2h
    delete_request_store: filesystem

singleBinary:
  replicas: 1
  resources:
    requests:
      cpu: 100m
      memory: 256Mi
    limits:
      cpu: 500m
      memory: 1Gi
  tolerations:
    - key: node-role.kubernetes.io/control-plane
      operator: Exists
      effect: NoSchedule
    - key: node-role.kubernetes.io/master
      operator: Exists
      effect: NoSchedule
  persistence:
    enabled: true
    storageClass: longhorn
    size: 10Gi

# D√©sactiver les composants microservices (on est en SingleBinary)
backend:
  replicas: 0
read:
  replicas: 0
write:
  replicas: 0

ingester:
  replicas: 0
querier:
  replicas: 0
queryFrontend:
  replicas: 0
queryScheduler:
  replicas: 0
distributor:
  replicas: 0
compactor:
  replicas: 0
indexGateway:
  replicas: 0
bloomCompactor:
  replicas: 0
bloomGateway:
  replicas: 0

# Gateway nginx (d√©sactiv√©, acc√®s direct au service Loki)
gateway:
  enabled: false

# Monitoring
serviceMonitor:
  enabled: true
  labels: {}

# D√©sactiver les tests et le chunksCache/resultsCache
test:
  enabled: false
lokiCanary:
  enabled: false
chunksCache:
  enabled: false
resultsCache:
  enabled: false
EOF
```

### Cr√©er le values.yaml Promtail

```bash
cat > infrastructure/loki/promtail-values.yaml <<'EOF'
# Configuration Promtail pour K3s 3 masters

config:
  clients:
    - url: http://loki.loki.svc:3100/loki/api/v1/push

  snippets:
    pipelineStages:
      - cri: {}
      - labeldrop:
          - filename
      - match:
          selector: '{namespace=~"kube-system|longhorn-system|traefik|cert-manager|argocd|kyverno|monitoring|loki"}'
          stages:
            - static_labels:
                log_type: infrastructure
      - match:
          selector: '{namespace!~"kube-system|longhorn-system|traefik|cert-manager|argocd|kyverno|monitoring|loki"}'
          stages:
            - static_labels:
                log_type: application

tolerations:
  - key: node-role.kubernetes.io/control-plane
    operator: Exists
    effect: NoSchedule
  - key: node-role.kubernetes.io/master
    operator: Exists
    effect: NoSchedule

resources:
  requests:
    cpu: 50m
    memory: 64Mi
  limits:
    cpu: 200m
    memory: 256Mi

serviceMonitor:
  enabled: true
  labels: {}
EOF
```

> **Note sur les pipeline stages Promtail :** Le label `log_type` est ajout√© automatiquement pour distinguer les logs d‚Äôinfrastructure des logs applicatifs. Cela permet de filtrer rapidement dans Grafana. Le stage `labeldrop: filename` r√©duit la cardinalit√© des labels pour √©conomiser les ressources de Loki.
> 

### Cr√©er les Applications ArgoCD

```bash
cat > argocd-apps/loki.yaml <<EOF
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: loki
  namespace: argocd
  finalizers:
    - resources-finalizer.argocd.argoproj.io
spec:
  project: default

  sources:
    - repoURL: 'https://grafana.github.io/helm-charts'
      targetRevision: 6.29.0
      chart: loki
      helm:
        releaseName: loki
        valueFiles:
          - '\$values/infrastructure/loki/values.yaml'

    - repoURL: '$GIT_URL'
      targetRevision: main
      ref: values

  destination:
    server: 'https://kubernetes.default.svc'
    namespace: loki

  syncPolicy:
    automated:
      prune: true
      selfHeal: true
      allowEmpty: false
    syncOptions:
      - CreateNamespace=true
      - ServerSideApply=true
    retry:
      limit: 5
      backoff:
        duration: 5s
        factor: 2
        maxDuration: 3m
EOF

cat > argocd-apps/promtail.yaml <<EOF
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: promtail
  namespace: argocd
  finalizers:
    - resources-finalizer.argocd.argoproj.io
spec:
  project: default

  sources:
    - repoURL: 'https://grafana.github.io/helm-charts'
      targetRevision: 6.16.6
      chart: promtail
      helm:
        releaseName: promtail
        valueFiles:
          - '\$values/infrastructure/loki/promtail-values.yaml'

    - repoURL: '$GIT_URL'
      targetRevision: main
      ref: values

  destination:
    server: 'https://kubernetes.default.svc'
    namespace: loki

  syncPolicy:
    automated:
      prune: true
      selfHeal: true
      allowEmpty: false
    syncOptions:
      - CreateNamespace=true
      - ServerSideApply=true
    retry:
      limit: 5
      backoff:
        duration: 5s
        factor: 2
        maxDuration: 3m
EOF
```

---

### √âtape 28 ‚Äî D√©ployer Loki et Promtail via ArgoCD

**Depuis votre poste de travail.**

### Pousser et d√©ployer

```bash
cd ~/k3s-gitops
git add infrastructure/loki/ argocd-apps/loki.yaml argocd-apps/promtail.yaml
git commit -m "Add Loki and Promtail for log aggregation"
git push origin main

# D√©ployer Loki en premier (Promtail a besoin du service Loki)
kubectl apply -f argocd-apps/loki.yaml

# Attendre que Loki soit Running
kubectl get pods -n loki -w
# Attendre que le pod loki-0 soit Running (2-3 minutes)

# Puis d√©ployer Promtail
kubectl apply -f argocd-apps/promtail.yaml

kubectl get application -n argocd loki -w
kubectl get application -n argocd promtail -w
```

### V√©rification

```bash
# Pods Loki (1 pod StatefulSet)
kubectl get pods -n loki
# loki-0 doit √™tre Running

# Pods Promtail (3 pods DaemonSet, un par n≈ìud)
kubectl get daemonset -n loki
# DESIRED=3, CURRENT=3, READY=3

# PVC Loki (doit √™tre Bound)
kubectl get pvc -n loki

# V√©rifier que Loki est accessible
kubectl exec -n loki loki-0 -c loki -- wget -qO- http://localhost:3100/ready
# R√©sultat attendu : ready

# V√©rifier que Promtail envoie des logs
kubectl logs -n loki -l app.kubernetes.io/name=promtail --tail=5

# R√©sultat attendu : des lignes mentionnant l'envoi de logs vers Loki
```

### Tester dans Grafana

1. Ouvrir `https://grafana.votredomaine.com`
2. Menu lat√©ral ‚Üí Data Source
3. Add new Data source
4. Loki
5. Connection URL : `http://loki.loki.svc:3100`
6. Menu lat√©ral ‚Üí Explore
7. S√©lectionner la datasource **Loki**
8. Entrer une requ√™te LogQL :

```
{namespace="kube-system"}
```

R√©sultat attendu : les logs des pods kube-system s‚Äôaffichent.

Exemples de requ√™tes LogQL utiles :

```
# Tous les logs d'un namespace
{namespace="argocd"}

# Logs d'un pod sp√©cifique
{namespace="traefik", pod=~"traefik-.*"}

# Filtrer par contenu
{namespace="cert-manager"} |= "error"

# Logs d'infrastructure vs applicatifs
{log_type="infrastructure"}
{log_type="application"}

# Logs avec parsing JSON
{namespace="argocd"} | json | level="error"

# Compteur de logs d'erreur par namespace (m√©trique)
sum by (namespace) (count_over_time({log_type="infrastructure"} |= "error" [5m]))
```

---

## Op√©rations courantes

### Backup etcd

```bash
# Sur n'importe quel master
sudo k3s etcd-snapshot save
sudo k3s etcd-snapshot ls

# Copier en dehors du cluster (recommand√©)
sudo cp /var/lib/rancher/k3s/server/db/snapshots/<snapshot> /backup/
```

### Mise √† jour K3s

Proc√©der un n≈ìud √† la fois, en commen√ßant par les n≈ìuds secondaires. Attendre que chaque n≈ìud soit `Ready` avant de passer au suivant.

```bash
k3s --version
curl -sfL https://get.k3s.io | INSTALL_K3S_VERSION=v1.31.x+k3s1 sh -s - server
kubectl get nodes
```

### Nettoyer les images inutilis√©es

```bash
# Sur chaque n≈ìud
sudo k3s crictl rmi --prune
```

### Ajouter un namespace applicatif avec NetworkPolicies

1. Copier `infrastructure/network-policies/app-namespace-template.yaml`
2. Remplacer `mon-namespace` par le nom du namespace
3. Adapter les labels et ports selon votre application
4. Ajouter le fichier dans `infrastructure/network-policies/kustomization.yaml`
5. Git add, commit, push ‚Äî ArgoCD synchronise automatiquement

### Consulter les rapports Kyverno

```bash
# Voir les PolicyReports par namespace
kubectl get policyreport -A

# D√©tail d'un rapport
kubectl describe policyreport -n <namespace>
```

---

## Ressources

- [K3s](https://docs.k3s.io/) ¬∑ [Longhorn](https://longhorn.io/docs/) ¬∑ [ArgoCD](https://argo-cd.readthedocs.io/) ¬∑ [Traefik](https://doc.traefik.io/traefik/) ¬∑ [cert-manager](https://cert-manager.io/docs/) ¬∑ [Kyverno](https://kyverno.io/docs/) ¬∑ [Prometheus](https://prometheus.io/docs/) ¬∑ [Grafana](https://grafana.com/docs/) ¬∑ [kube-prometheus-stack](https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-prometheus-stack) ¬∑ [Loki](https://grafana.com/docs/loki/latest/) ¬∑ [Promtail](https://grafana.com/docs/loki/latest/send-data/promtail/) ¬∑ [LogQL](https://grafana.com/docs/loki/latest/query/) ¬∑ [Helm](https://helm.sh/docs/) ¬∑ [Kustomize](https://kustomize.io/) ¬∑ [Tailscale](https://tailscale.com/kb/) ¬∑ [NetworkPolicies](https://kubernetes.io/docs/concepts/services-networking/network-policies/)